\part{Fermionic path integrals}

\chapter{Grassmann numbers and quantisation}

    In this chapter, we will study what Grassmann numbers are and how they can be used to describe classical phase space of fermions. We will also make the example of the fermionic harmonic oscillator and we will recall notions of bosonic theory.

\section{Grassmann numbers} 

    In order to find a way to study fermionic systems at a classical level and then quantisining it by means of path integrals, we need to take into account the antisymmetry of the wavefunction and the Pauli exclusion principle. Therefore, we introduce new numbers, called Grassmann numbers, that satisfy the anticommutation relation. Keep in mind that the notion of spin is not classical and therefore these kind of systems are called pseudoclassical.
    
    An $n$-dimensional Grassmann algebra $\mathfrak G_n$ is generated by a set of generators $\theta_i$, called Grassmann numbers. They satisfy the anticommutation relation and the Pauli exclusion principle
    \begin{equation}\label{f:anti}
        \{\theta_i, \theta_j\} = \theta_i \theta_j + \theta_j \theta_i = 0 ~, \quad (\theta_i)^2 = \theta_i \theta_i = 0 ~.
    \end{equation}

    It is possible to multiply and add Grassmann numbers to build polynomial functions. For a single variable $(n = 1)$, we can have a term in $\theta$, by the second of~\eqref{f:anti}. Therefore, a function $f$ can be expressed only by giving two numbers $f_0$ and $f_1$
    \begin{equation*}
        f(\theta) = f_0 + f_1 \theta ~,
    \end{equation*}
    When we have more variables $(n=2)$, we have single terms in $\theta_1$ and $\theta_2$, such that each $\theta_i$ satisfies the second of~\eqref{f:anti}, and add the mixing term $\theta_1 \theta_2$, such that satisfies the first of~\eqref{f:anti}. The function $f$ can be expressed as by giving four numbers $f_0$, $f_1$, $f_2$, $f_3$
    \begin{equation*} 
        f(\theta_1, \theta_2) = f_0 + f_1 \theta_1 + f_2 \theta_2 + f_3 \theta_1 \theta_2 ~.
    \end{equation*} 
    The term $\theta_2 \theta_1$ is not written since it is not independent by the anticommutation relation. In fact, we should have written something like 
    \begin{equation*}
        a \theta_1 \theta_2 + b \underbrace{\theta_2 \theta_1}_{-\theta_1 \theta_2} = a \theta_1 \theta_2 - b \theta_1 \theta_2 = \underbrace{(a - b)}_{f_3} \theta_1 \theta_2 = f_3 \theta_1 \theta_2 ~.
    \end{equation*}
    Terms with an even number of Grassmann variables are called Grassmann even and terms with an odd number of Grassmann variables are called Grassmann odd. A generic function of Grassmann variables can be defined by its Taylor expansion. 
    \begin{example}
        The exponential is given by
        \begin{equation*}
            \exp(\theta) = 1 + \theta ~,
        \end{equation*}
        where $f_0 = 1$ and $f_1 = 1$.
    \end{example}
    
    Derivatives are simple since all the functions are linear in Grassmann variables. However, we can define $2$ kind of derivatives: left and right. The only difference, by the anticommutativity relation, is only a minus sign. For example in $n = 2$, we have 
    \begin{equation*}
        \pdv{_l}{\theta_1} f(\theta_1, \theta_2) = \pdv{_l}{\theta_1} (f_0 + f_1 \theta_1 + f_2 \theta_2 + f_3 \theta_1 \theta_2) = f_1 + f_3 \theta_2 ~,
    \end{equation*}
    \begin{equation*}
    \begin{aligned}
        \pdv{_r}{\theta_1} f(\theta_1, \theta_2) & = \pdv{_r}{\theta_1} (f_0 + f_1 \theta_1 + f_2 \theta_2 + f_3 \theta_1 \theta_2) \\ & = \pdv{_r}{\theta_1} (f_0 + f_1 \theta_1 + f_2 \theta_2 - f_3 \theta_2 \theta_1) = f_1 - f_3 \theta_2 ~.
    \end{aligned}
    \end{equation*}
    In these notes, we will use always left derivatives, if not otherwise indicated. Grassmann increments are given by 
    \begin{equation*}
        \delta f = \pdv{_r f}{\theta} \delta \theta = \delta \theta \pdv{_l f}{\theta} ~.
    \end{equation*}
    In fact, we have for $n=2$, the left derivative
    \begin{equation*}
    \begin{aligned}
        \delta f & = \pdv{_l f}{\theta_1} \delta \theta_1 + \pdv{_l f}{\theta_2} \delta \theta_2 = (f_1 + f_3 \theta_2) \delta \theta_1 + (f_1 + f_3 \theta_1) \delta \theta_2 \\ & = f_1 \delta \theta_1 + f_3 \theta_2 \delta \theta_1 + f_1 \delta \theta_2 + f_3 \theta_1 \delta \theta_2 ~,
    \end{aligned}
    \end{equation*}
    and the right derivative
    \begin{equation*}
    \begin{aligned}
        \delta f & = \delta \theta_1 \pdv{_r f}{\theta_1} + \delta \theta_2 \pdv{_r f}{\theta_2} = \delta \theta_1 (f_1 - f_3 \theta_2) + \delta \theta_2 (f_1 - f_3 \theta_1) \\ & = f_1 \delta \theta_1 - f_3 \underbrace{\delta \theta_1 \theta_2}_{- \theta_2 \delta \theta_1} + f_1 \delta \theta_2 - f_3 \underbrace{\delta \theta_2 \theta_1}_{- \theta_1 \delta \theta_2} = f_1 \delta \theta_1 + f_3 \theta_2 \delta \theta_1 + f_1 \delta \theta_2 + f_3 \theta_1 \delta \theta_2 ~.
    \end{aligned}
    \end{equation*}
    
    Berezin integrals are defined to be identical with derivatives
    \begin{equation*}
        \int d\theta = \pdv{}{\theta} ~.
    \end{equation*}
    It is translational invariant
    \begin{equation}\label{f:trans}
        \int d\theta ~ f(\theta + \eta) = \int d\theta ~ f(\theta) ~,
    \end{equation}
    where $\theta, \eta \in \mathfrak G$.
    \begin{proof}
        In fact 
        \begin{equation*}
            \int d\theta ~ f(\theta + \eta) = \pdv{}{\theta} f(\theta + \eta) = \pdv{}{\theta} (f_0 + f_1 (\theta + \eta)) = f_1 = \int d\theta ~ f(\theta) ~.
        \end{equation*}
    \end{proof}
    As a consequence, we have 
    \begin{equation*}
        \int d\theta ~ f(\theta + \eta) = \int d(\theta - \eta) ~ f(\theta + \eta) = \int d\theta' ~ f(\theta') ~,
    \end{equation*}
    where $\theta' = \theta - \eta$ and $d\theta' = d (\theta - \eta)$.
    \begin{proof}
        In fact, using~\eqref{f:trans}
        \begin{equation*}
            \int d\theta ~ f(\theta + \eta) = \int d(\theta - \eta) ~ f(\theta) = \int d(\theta - \eta) ~ f(\theta - \eta) = \int d\theta' ~ f(\theta') ~.
        \end{equation*}
    \end{proof}

    A Grassmann variable can be either real, i.e. $\theta = \theta^*$, or complex, i.e. $\theta \neq \theta^*$. The complex conjugate of the product of two Grassmann variables is 
    \begin{equation*}
        (\theta_1 \theta_2)^* = \theta_2^* \theta_1^* ~.
    \end{equation*}
    The complex conjugate of the product of $2$ real Grassmann variables $\theta_1^* = \theta_1$ and $\theta_2^* = \theta_2$ is purely imaginary 
    \begin{equation*}
        (\theta_2 \theta_1)^* = \theta_1^* \theta_2^* = \theta_1 \theta_2 = - \theta_2 \theta_1 ~,
    \end{equation*}
    whereas if we multiply by $i$ we obtain a real Grassmann variable 
    \begin{equation*}
        (i \theta_2 \theta_1)^* = - i\theta_1^* \theta_2^* = - i \theta_1 \theta_2 = i \theta_2 \theta_1 ~.
    \end{equation*}
    We can always express complex Grassmann variables $\eta$ in terms of real ones $\theta_1$ and $\theta_2$
    \begin{equation*}
        \eta = \frac{\theta_1 + i \theta_2}{\sqrt 2} ~, \quad \eta^* = \frac{\theta_1 - i \theta_2}{\sqrt 2} ~.
    \end{equation*}
    These definitions are important, since in quantum mechanics, we want hermitian operators.

    The kind of integrals necessary for the computation of path integrals are Gaussian integrals. For a single Grassmann variable $\theta$, it is is trivial, since 
    \begin{equation*}
        \exp(a \theta^2) = 1 + a \underbrace{\theta^2}_0 = 1 
    \end{equation*}
    and
    \begin{equation*}
        \int d\theta ~ \exp(a \theta^2) = \int d\theta ~ 1 = \pdv{}{\theta} 1 = 0 ~.
    \end{equation*}
    We need at least $2$ Grassmann variables $\theta_1$ and $\theta_2$ to have a non trivial Gaussian integral, since 
    \begin{equation*}
        \exp(- a \theta_1 \theta_2) = 1 - a \theta_2 \theta_2 
    \end{equation*}
    and 
    \begin{equation*}
    \begin{aligned}
        \int d\theta_1 d \theta_2 ~ \exp(- a \theta_1 \theta_2) & = \pdv{}{\theta_1} \pdv{}{\theta_2} (1 - a \theta_1 \theta_2) = \pdv{}{\theta_1} \pdv{}{\theta_2} (1 + a \theta_2 \theta_1 ) \\ & = \pdv{}{\theta_1} (a \theta_1) = a ~.
    \end{aligned}
    \end{equation*}
    Notice that when we integrated over a $2$-dimensional measure, there is a precisely order to compute integrals 
    \begin{equation*}
        \int d \theta_1 d \theta_2 = - \int d\theta_2 d \theta_1 ~. 
    \end{equation*}
    We can rewritten the Gaussian integral in terms of a skew-symmetric $2 \times 2$ matrix $A^{ij}$, defined as 
    \begin{equation*}
        A = \begin{bmatrix}
            0 & a \\ -a & 0 \\
        \end{bmatrix} ~, \quad \det A = a^2 \geq 0 ~,
    \end{equation*}
    so that it becomes 
    \begin{equation*}
        \int d\theta_1 d \theta_2 ~ \exp(- \frac{1}{2} \theta_i A^{ij} \theta_j) = {\det}^{1/2} A^{ij} ~,
    \end{equation*}
    where the square root of athe determinant of a skew-symmetric matrix is called the pfaffian. Recall that in the bosonic case, we had $\det^{- 1/2}$. Generalising for an even number of Grassmann variables $n = 2m$ for $m \in \mathbb N$, we have 
    \begin{equation*}
        \int d^n \theta ~ \exp(- \frac{1}{2} \theta_i A^{ij} \theta_j) = {\det}^{1/2} A^{ij} ~,
    \end{equation*}
    where $d^n \theta = d\theta_1 \ldots d\theta_n$. $A$ is a skew-symmetric matrix and skew-symmetric matrix can be always put in block-diagonal form by an orthogonal transformation, which leaves the measure invariant, where each block is skew-symmetric 
    \begin{equation*}
        A = \begin{bmatrix}
            0 & -a_1 & \ldots & \ldots & \ldots \\
            - a_1 & 0 & \ldots & \ldots & \ldots \\
            \ldots & \ldots & \ldots & \ldots & \ldots \\
            \ldots &  \ldots & \ldots & 0 & a_n \\
            \ldots &  \ldots & \ldots & - a_n & 0 \\
        \end{bmatrix} ~,
    \end{equation*}
    whose determinant is ${\det}^{1/2} A = a_1 \ldots a_n$. It is important to highlight the fact that for a change of variables, we need to compute the jacobian, which is the inverse with respect to the bosonic case, since integration is defined as derivation. We can generalise for complex variables $\eta_i$ and have 
    \begin{equation*}
        \int d^n \eta^* d^n \eta ~ \exp(- \frac{1}{2} \eta_i^* A^{ij} \eta_j) = \det A^{ij} ~,
    \end{equation*}
    where $d^n \eta^* d^n \eta = d \eta_1^* d \eta_1 \ldots d \eta_n^* d \eta_n$.

    In order to find physical application, we consider an infinite-dimensional Grassman algebra $\mathfrak G_\infty$, where we have a Grassmann variable at each instant of time $\theta_i \rightarrow \theta (t)$. The properties of~\eqref{f:anti} becomes 
    \begin{equation*}
        \theta^2 (t) = 0 ~, \quad \theta(t_1) \theta(t_2) = - \theta(t_2) \theta(t_1) = 0 ~.
    \end{equation*}

\section{Fermionic harmonic oscillator}

    Consider an bosonic harmonic oscillator with mass $m = 1$ and frequency $\omega$. Its action is 
    \begin{equation}\label{f:harmac}
        S[x, p] = \int dt ~ (p \dot x - \frac{1}{2} (p^2 + \omega^2 x^2)) ~.
    \end{equation}
    It is possible to express the action, by a suitable change of basis, in terms of the annihilation and the creation operators, defined as
    \begin{equation}\label{f:ladd}
        a = \frac{\omega x + i p}{\sqrt{2 \omega}} ~, \quad a^* = \frac{\omega x - i p}{\sqrt{2 \omega}} ~, 
    \end{equation}
    hence, we find
    \begin{equation*}
        S[a, a^*] = \int dt (i a^* \dot a - \omega a^* a) + \textnormal{boundary terms} ~.
    \end{equation*}
    We promote it to operators $\hat a$ and $\hat a^\dagger$ via the commutation relations $[\hat a, \hat a^\dagger] = 1$ living in a Fock space. 
    \begin{proof}
        We invert~\eqref{f:ladd}, by adding
        \begin{equation*}
            a + a^* = \frac{2 \omega}{\sqrt{2 \omega}} x ~, \quad x = \frac{1}{\sqrt{2\omega}} (a + a^*) ~,
        \end{equation*}
        and subtracting 
        \begin{equation*}
            a - a^* = \frac{2i}{\sqrt{2 \omega}} p ~, \quad p = - i \sqrt{\frac{\omega}{2}} (a - a^*) ~.
        \end{equation*}
        Useful intermediary formulas are 
        \begin{equation*}
            \dot x = \frac{1}{\sqrt{2\omega}} (\dot a + \dot a^*) ~,
        \end{equation*}
        \begin{equation*}
            x^2 = \frac{1}{2 \omega} (a + a^*)^2 = \frac{1}{2 \omega} (a^2 + 2 a^* a + (a^*)^2) 
        \end{equation*}
        and
        \begin{equation*}
            p^2 = - \frac{\omega}{2} (a - a^*)^2 = - \frac{\omega}{2} (a^2 - 2 a^* a + (a^*)^2)  ~.
        \end{equation*}
        Therefore, we substitute them in the action~\eqref{f:harmac} 
        \begin{equation*}
        \begin{aligned}
            S & = \int dt ((- i \sqrt{\frac{\omega}{2}} (a - a^*))(\frac{1}{\sqrt{2\omega}} (\dot a + \dot a^*)) \\ & \quad - \frac{1}{2} (- \frac{\omega}{2} (a^2 - 2 a^* + (a^*)^2) + \omega^2 \frac{1}{2 \omega} (a^2 + 2 a^* a + (a^*)^2) )) \\ & = \int dt \Big  (- \frac{i}{2} (a \dot a + a \dot a^* - a^* \dot a - a^* \dot a^*) \\ & \quad - \frac{\omega}{4} (- a^2 +2a^* a - (a^*)^2 + a^2 + 2a^* a + (a^*)^2) \Big ) \\ & = \int dt \Big  (\frac{i}{2} ( - a \dot a - \underbrace{a \dot a^*}_{-a^* \dot a + \dv{}{t} (a a^*)} + a^* \dot a + a^* \dot a^*) - \frac{\omega}{4} (4 a a^*) \Big ) \\ & = \int dt (i a^* \dot a - \omega a^* a) + \int dt (\dv{}{t} (a a^*) - a \dot a + a^* \dot a^*) ~,
        \end{aligned}
        \end{equation*}
        where the last term is a boundary term 
        \begin{equation*}
        \begin{aligned}
            - a \dot a + a^* \dot a^* & = - \frac{1}{2 \omega} (\omega x + i p) (\omega \dot x + i \dot p) + \frac{1}{2 \omega} (\omega x - i p) (\omega \dot x - i \dot p) \\ & = \frac{1}{2\omega} (- \omega^2 x \dot x - i \omega x \dot p - i \omega \dot x p + p \dot p + \omega^2 x \dot x - i \omega x \dot p - i \omega \dot x p - p \dot p) \\ & = \frac{1}{2 \omega} (- 2 i \omega x \dot p - 2 i \omega \dot x p) = - i\dv{}{t} (xp) ~.
        \end{aligned}
        \end{equation*}
    \end{proof}

    Now, we consider a fermionic harmonic oscillator. It is described by complex Grassmann variables $\psi(t)$ and $\psi^*(t)$, which satisfy the relations~\eqref{f:trans} 
    \begin{equation*}
        \psi(t) \psi(t) = 0 ~, \quad \psi^* (t) \psi^* (t) = 0 ~, \quad \dot \psi(t) \dot \psi(t) = 0 ~, \quad \dot \psi^* (t) \dot \psi^* (t) = 0 ~,
    \end{equation*}
    \begin{equation*}
        \psi(t) \psi(t') = - \psi (t') \psi (t) ~, \quad \psi^* (t) \psi^* (t') = - \psi^* (t') \psi^* (t) ~,
    \end{equation*}
    \begin{equation*}
        \dot \psi(t) \dot \psi(t') = - \dot \psi (t') \dot \psi (t) ~, \quad \dot \psi^* (t) \dot \psi^* (t') = - \dot \psi^* (t') \dot \psi^* (t) ~,
    \end{equation*}
    \begin{equation*}
        \psi(t) \dot \psi(t) = - \dot \psi (t) \psi (t) ~, \quad \psi(t) \psi^* (t) = - \psi^* (t) \psi (t) ~,
    \end{equation*}
    \begin{equation*}
        \psi(t) \dot \psi^*(t) = - \dot \psi^* (t) \psi (t) ~, \quad \psi^*(t) \dot \psi^* (t) = - \dot \psi^* (t) \psi^* (t) ~,
    \end{equation*}
    \begin{equation*}
        \dot \psi(t) \dot \psi^* (t) = - \dot \psi^* (t) \dot \psi (t) ~.
    \end{equation*}
    By analogy with the bosonic case, the action can be written as
    \begin{equation*}
        S[\psi, \psi^*] = \int dt ~ (i \psi^* \dot \psi - \omega \psi^* \psi)  ~.
    \end{equation*}
    The action is real and its equation of motion is 
    \begin{equation*}
        i \dot \psi - \omega \psi = 0 ~.
    \end{equation*}
    Notice that it is similar to the time components of the Dirac equation $(\gamma^0 \partial_0 + m)\psi = 0$, with $m = \omega$ and $\gamma^0 = - i$. A possible solution is a plane wave 
    \begin{equation*}
        \psi(t) = \psi_0 \exp(- i \omega t) ~,
    \end{equation*}
    where $\psi_0$ is given by the initial conditions.
    \begin{proof}
        In fact, by the Euler-Lagrange equations 
        \begin{equation*}
            0 = \pdv{L}{\psi^*} - \dv{}{t} \pdv{L}{\dot \psi^*} = i \dot \psi - \omega \psi ~.
        \end{equation*}
    \end{proof}

    Now, we use the canonical quantisation method in phase space. The conjugate momentum is 
    \begin{equation*}
        \pi = \pdv{_l L}{\dot \psi} = \pdv{_l}{\dot \psi} (i \psi^* \dot \psi - \omega \psi^* \psi) = \pdv{_l}{\dot \psi} ( - i \dot \psi \psi^*) = - i \psi^* ~,
    \end{equation*}
    which shows that the system is already in Hamiltonian form, up to a factor $-i$. The classical Poisson brackets are 
    \begin{equation*}
        \{\pi, \psi\}_{PB} = - 1 ~, \quad \{\psi, \psi^*\}_{PB} = - i ~,
    \end{equation*}
    which are symmetric. Quantisining with anticommutators, since they are fermions, we have  
    \begin{equation*}\label{f:antic}
        \{\hat \psi, \hat \psi^\dagger\} = 1 ~, \quad \{\hat \psi, \hat \psi\} = \{\hat \psi^\dagger, \hat \psi^\dagger\} = 0 ~.
    \end{equation*}
    which is realised by a $2$-dimensional Hilbert space. By Fock construction, we can build the space in which states live, i.e. the irreducible representation for the Grassmann algebra. The vacuum $\ket{0}$ is the state such that 
    \begin{equation*}
        \hat \psi \ket{0} = \bra{0} \hat \psi^\dagger = 0 ~, \quad \braket{0}{0} = 1 ~.
    \end{equation*}
    The first excited state $\ket{1}$ is the state such that
    \begin{equation*}
        \ket{1} = \hat \psi^\dagger \ket{0} ~, \quad \bra{1} = \bra{0} \hat \psi ~, \quad \braket{0}{1} = \bra{0} \hat \psi^\dagger \ket{0} = 0 ~,
    \end{equation*}
    \begin{equation*}
        \braket{1}{1} = \bra{0} \hat \psi \hat \psi^\dagger \ket{0} = \bra{0} \underbrace{\{\hat \psi, \hat \psi^\dagger\}}_1 \ket{0} + \bra{0} \hat \psi^\dagger \underbrace{\hat \psi \ket{0}}_0 = \braket{0}{0} = 1 ~.
    \end{equation*}
    Notice that we cannot build anymore kets, since we would have 
    \begin{equation*}
        \ket{2} = \underbrace{(\hat \psi^\dagger)^2}_0 \ket{0} = 0 ~.
    \end{equation*}
    Therefore, the Fock space is 
    \begin{equation*}
        \mathcal F_2 = \spann \{\ket{0}, \ket{1} \}
    \end{equation*}
    and its orthonormal basis is $\braket{m}{n} = \delta_{mn}$, with $m, n = 0, 1$. We can write the operators $\hat \psi$ and $\hat \psi^\dagger$ in matrix notation as
    \begin{equation*}
        \hat \psi = \begin{bmatrix}
            \bra{0} \underbrace{\hat \psi \ket{0}}_0 & \bra{0} \underbrace{\hat \psi \ket{1}}_{\ket{0}} \\ \bra{1} \underbrace{\hat \psi \ket{0}}_0 & \bra{1} \underbrace{\hat \psi \ket{1}}_{\ket{0}} \\
        \end{bmatrix} = \begin{bmatrix}
            0 & \underbrace{\braket{0}{0}}_1 \\ 0 & \underbrace{\braket{1}{0}}_0 \\
        \end{bmatrix} = \begin{bmatrix}
            0 & 1 \\ 0 & 0 \\
        \end{bmatrix} ~,
    \end{equation*}
    \begin{equation*}
        \hat \psi^\dagger = \begin{bmatrix}
            \bra{0} \underbrace{\hat \psi^\dagger \ket{0}}_{\ket{1}} & \bra{0} \underbrace{\hat \psi^\dagger \ket{1}}_0 \\ \bra{1} \underbrace{\hat \psi^\dagger \ket{0}}_{\ket{1}} & \bra{1} \underbrace{\hat \psi^\dagger \ket{1}}_0 \\
        \end{bmatrix} = \begin{bmatrix}
            \underbrace{\braket{0}{1}}_0 & \\  \underbrace{\braket{1}{1}}_1 &  0 \\
        \end{bmatrix} = \begin{bmatrix}
            0 & 0 \\ 1 & 0 \\
        \end{bmatrix} ~.
    \end{equation*}
    Hence, they satisfy the anticommutation relations~\eqref{f:antic}.
    \begin{proof}
        For the first, 
        \begin{equation*}
            \{\hat \psi, \hat \psi\} = \hat \psi \hat \psi + \hat \psi \hat \psi = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} = 0 + 0 = 0 ~.
        \end{equation*}
        For the second, 
        \begin{equation*}
            \{\hat \psi^\dagger, \hat \psi^\dagger \} = \hat \psi^\dagger \hat \psi^\dagger + \hat \psi^\dagger \hat \psi^\dagger = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 0 \\ 1 & 0\end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix}
                0 & 0 \\ 0 & 0 \\
            \end{bmatrix} ~.
        \end{equation*}
        For the third, 
        \begin{equation*}
            \{\hat \psi, \hat \psi^\dagger \} = \hat \psi \hat \psi^\dagger + \hat \psi^\dagger \hat \psi = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 0 \\ 1 & 0\end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix}
                1 & 0 \\ 0 & 1 \\
            \end{bmatrix} ~.
        \end{equation*}
    \end{proof}
    The Hamiltonian becomes 
    \begin{equation*}
        \hat H = \frac{\omega}{2} (\hat \psi^\dagger \hat \psi - \frac{1}{2}) ~,
    \end{equation*}
    where order matters. The Fock vacumm is indeed the ground state.
    \begin{proof}
        By a Legendre transform 
        \begin{equation*}
            H = \dot \psi \pi - L = - i \dot \psi \psi^* - i \psi^* \dot \psi + \omega \psi^* \psi = - i \dot \psi \psi^* + i \dot \psi \psi^* + \omega \psi^* \psi = \omega \psi^* \psi ~,
        \end{equation*}
        where we have used the order $\dot \psi \pi$ because we are using left derivatives. We can rewritten it as 
        \begin{equation*}
            H = \frac{\omega}{2} (\psi^* \psi - \psi \psi^*) = \frac{\omega}{2} (\psi^* \psi + \psi^* \psi) = \omega \psi^* \psi ~.
        \end{equation*}
        We quantise the last expression of the Hamiltonian and we find 
        \begin{equation*}
            \hat H = \frac{\omega}{2} (\hat \psi^\dagger \hat \psi - \hat \psi \hat \psi^\dagger) = \frac{\omega}{2} (\hat \psi^\dagger \hat \psi - \{\hat \psi \hat, \psi^\dagger \}+ \hat \psi^\dagger \hat \psi) = \omega (\hat \psi^\dagger \hat \psi - \frac{1}{2}) ~.
        \end{equation*}
    \end{proof} 

\section{Hamiltonian structure of phase space}

    Now, we generalise the Hamiltonian formalism for a generic physical system described by Grassmann variables. Consider first a bosonic system with Hamiltonian action
    \begin{equation*}
        S[x, p] = \int dt ~ (p \dot x - H(x, p)) ~,
    \end{equation*}
    where the first term is the sympletic one, which gives rise to the Poisson brackets. In fact, we can rewritten the action as 
    \begin{equation*}
        S[z] = \int dt (\frac{1}{2} z^a (\Omega^{-1})_{ab} \dot z^b - H(z)) + \text{boundary terms} ~.
    \end{equation*}
    where we have called $z^a = (x, p)$ with $a = 1,2$. It is in first order in time, since Hamilton equations are, and $\Omega_{ab}$ is an invertible antisymmetric constant-valued matrix, which in canonical coordinates is 
    \begin{equation*}
        \Omega_{ab} = \begin{bmatrix}
            0 & 1  \\ -1 & 0 \\
        \end{bmatrix} ~, \quad (\Omega^{-1})_{ab} = \begin{bmatrix}
            0 & -1  \\ 1 & 0 \\
        \end{bmatrix} ~.
    \end{equation*}
    \begin{proof}
        In fact 
        \begin{equation*}
        \begin{aligned}
            S & = \int dt ~ (p \dot x - H(x, p)) = \int dt ~ (\frac{1}{2}p \dot x + \frac{1}{2} \underbrace{p \dot x}_{- x \dot p + \dv{}{t} (p x)} - H(x, p)) \\ & = \int dt ~ (\frac{1}{2} (p \dot x - x \dot p) - H(x, p)) + \int dt ~ (\dv{}{t} (xp)) ~.
        \end{aligned}
        \end{equation*}
        Hence,
        \begin{equation*}
            z^a (\Omega^{-1})_{ab} \dot z^b = \begin{bmatrix}
                x & p \\
            \end{bmatrix} \begin{bmatrix}
                0 & -1 \\ 1 & 0 \\
            \end{bmatrix} \begin{bmatrix}
                \dot x \\ \dot p \\
            \end{bmatrix} = \begin{bmatrix}
                x & p \\
            \end{bmatrix} \begin{bmatrix}
                - \dot p \\ \dot x \\
            \end{bmatrix} = p \dot x - x \dot p ~.
        \end{equation*}
    \end{proof}
    Given $2$ functions $f(z)$ and $g(z)$, the Poisson brackets are defined as 
    \begin{equation*}
        \{f(z), g(z)\}_{PB} = \pdv{f}{z^a} \Omega^{ab} \pdv{g}{z^b} ~.
    \end{equation*}
    In particular, we have 
    \begin{equation*}
        \{z^a, z^b\}_{PB} = \pdv{z^a}{z^a} \Omega^{ab} \pdv{z^b}{z^b} =  \Omega^{ab} ~.
    \end{equation*}
    It satisfies the following properties 
    \begin{enumerate}
        \item antisymmetry, i.e.
            \begin{equation*}
                \{f, g\}_{PB} = - \{g, f\}_{PB} ~,
            \end{equation*}
        \item Leibniz rule, i.e. 
            \begin{equation*}
                \{f, g h\}_{PB} = \{f, g\}_{PB} h + g \{f, h\}_{PB}~,
            \end{equation*}
        \item Jacobi identity, i.e. 
            \begin{equation*}
                \{f, \{g, h\}_{PB} \}_{PB} + \{g, \{h, f\}_{PB} \}_{PB} + \{h, \{f, g\}_{PB} \}_{PB} ~.
            \end{equation*}
    \end{enumerate}
    The Hamilton equations are 
    \begin{equation*}
        \dot z^a = \Omega^{ab} \pdv{H}{z^b} = \{z^a, H\}_{PB} ~.
    \end{equation*}
    In canonical quantisation, we promote the Poisson brackets to commutators of operators
    \begin{equation*}
        [\hat z^a, \hat z^b] = i \hbar \Omega^{ab} ~.
    \end{equation*}
    This definition is consistent, since both sides satisfy the properties of the Poisson brackets. We have found an irreducible representation of the operator algebra on a well-defined Hilbert space in which states live.

    Now, in order to study this prescription for Grassmann variables, we defined coordinates as
    \begin{equation*}
        Z^A = (z^a, \theta^\alpha) ~,
    \end{equation*}
    where $z^a$ are bosonic coordinates (Grassmann even) and  $\theta^\alpha$ are fermionic ones (Grassmann odd). The corresponding Hamiltonian action is 
    \begin{equation*}
        S[Z^A] = \int dt ~ \Big (\frac{1}{2} Z^A (\Omega^{-1})_{AB} \dot Z^B - H(Z) \Big) ~,
    \end{equation*}
    where all terms must be Grassmann evens so that the action is bosonic. It is in first order in time. The graded-sympletic matrix $\Omega^{AB}$ is 
    \begin{equation*}
        \Omega^{AB} = \begin{bmatrix}
            \Omega^{ab} & 0 \\ 0 & \Omega^{\alpha\beta} \\
        \end{bmatrix} ~, \quad \Omega^{ab} = \begin{bmatrix}
            0 & 1  \\ -1 & 0 \\
        \end{bmatrix} ~, \quad \Omega^{\alpha\beta} = \begin{bmatrix}
            0 & 1  \\ 1 & 0 \\
        \end{bmatrix} ~,
    \end{equation*}
    where the bosonic block is antisymmetric and the fermionic block is symmetric. There could not be mixing terms because they would be anticommuting and we want a commuting action.
    \begin{proof}
        If the bosonic block were symmetric, we would have a total derivative, which in the action it is a boundary term
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} x & p \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ 1 & 0
            \end{bmatrix} \begin{bmatrix} \dot x \\ \dot p \end{bmatrix} = p \dot x + \dot x p = \dv{(px)}{t} ~.
        \end{equation*}
        Instead, we have
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} x & p \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ -1 & 0
            \end{bmatrix} \begin{bmatrix} \dot x \\ \dot p \end{bmatrix} = p \dot x - \dot x p \neq \dv{(px)}{t}
        \end{equation*}
        If the fermionic block were antisymmetric, we would have a total derivative, which in the action it is a boundary term
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} \theta_1 & \theta_2 \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ -1 & 0
            \end{bmatrix} \begin{bmatrix} \dot \theta_1 \\ \dot \theta_1 \end{bmatrix} = \theta_1 \dot \theta_2 - \dot \theta_2 \theta_1 = \theta_1 \dot \theta_2 + \theta_1 \dot \theta_2 = \dv{(\theta_1 \theta_2)}{t} ~.
        \end{equation*}
        Instead, we have
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} \theta_1 & \theta_2 \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ 1 & 0
            \end{bmatrix} \begin{bmatrix} \dot \theta_1 \\ \dot \theta_1 \end{bmatrix} = \theta_1 \dot \theta_2 + \dot \theta_2 \theta_1 = \theta_1 \dot \theta_2 - \theta_1 \dot \theta_2 \neq \dv{(\theta_1 \theta_2)}{t} ~.
        \end{equation*}
    \end{proof}
    Given $2$ functions $f(Z)$ and $g(Z)$, the Poisson brackets are defined as 
    \begin{equation*}
        \{f(Z), g(Z)\}_{PB} = \pdv{_r f}{Z^A} \Omega^{AB} \pdv{_l g}{Z^B} ~.
    \end{equation*}
    In particular, we have 
    \begin{equation*}
        \{Z^A, Z^B\}_{PB} = \pdv{_r Z^A}{Z^A} \Omega^{AB} \pdv{Z^B}{Z^B} =  \Omega^{AB} ~.
    \end{equation*}
    Defining the Grassmann parity 
    \begin{equation*}
        \epsilon_f = \begin{cases}
            0 & \textnormal{if f is bosonic} \\
            1 & \textnormal{if f is fermionic} \\
        \end{cases} ~,
    \end{equation*}
    they satisfies the following properties 
    \begin{enumerate}
        \item graded antysymmetry, i.e.
            \begin{equation*}
                \{f, g\}_{PB} = (-1)^{\epsilon_f \epsilon_g + 1} \{g, f\}_{PB} ~,
            \end{equation*}
        \item Leibniz rule, i.e. 
            \begin{equation*}
                \{f, g h\}_{PB} = \{f, g\}_{PB} h + (-1)^{\epsilon_f \epsilon_g} g \{f, h\}_{PB}~,
            \end{equation*}
        \item Jacobi identity, i.e. 
            \begin{equation*}
                \{f, \{g, h\}_{PB} \}_{PB} + (-1)^{\epsilon_f (\epsilon_g + \epsilon_h)} \{g, \{h, f\}_{PB} \}_{PB} + (-1)^{\epsilon_h (\epsilon_f + \epsilon_g)} \{h, \{f, g\}_{PB} \}_{PB} ~.
            \end{equation*}
    \end{enumerate}
    The Hamilton equations are 
    \begin{equation*}
        \dot Z^A = \Omega^{AB} \pdv{H}{Z^B} = \{Z^A, H\}_{PB} ~.
    \end{equation*}
    In canonical quantisation, we promote the Poisson brackets to graded commutators of operators
    \begin{equation*}
        [\hat Z^A, \hat Z^B\} = i \hbar \Omega^{AB} ~.
    \end{equation*}
    where the graded commutator is defined as 
    \begin{equation*}
        [\hat A, \hat B\} = \begin{cases}
            [\hat A, \hat B] & \text{if at least there is one bosonic operator} \\
            \{\hat A, \hat B\} & \text{if both are fermionic operators} \\
        \end{cases} ~.
    \end{equation*}

    \begin{example}
        Consider a single real Grassmann variable $\psi$, called also a Maiorana fermion in $0 + 1$ dimension. The action is 
        \begin{equation*}
            S[\psi] = \int dt ~ \frac{i}{2} \psi \dot \psi ~,
        \end{equation*}
        where we have put $i$ in order to have a real action and no hamiltonian because the only most simple quadratic term vanishes $\psi^2 = 0$. Therefore, the graded sympletic action is $\Omega^{-1} = i$ or $\Omega = -i$ and the canonical quantised anticommutator is 
        \begin{equation*}
            \{\hat \psi, \hat \psi\} = i \hbar (-i) = \hbar ~.
        \end{equation*}
        Finally, the only possibility is that $\hat \psi$ is a number
        \begin{equation*}
            \hat \psi = \sqrt{\frac{\hbar}{2}} ~.
        \end{equation*}
        The Fock space is consistuted by only the vacuum state $\ket{0}$. 
    \end{example}

    \begin{example}
        Consider $n$ real Grassmann variables $\psi^i$. The action is 
        \begin{equation*}
            S[\psi] = \int dt \frac{i}{2} \psi^i \delta_{ij} \dot \psi^j - H(\psi^i) ~.
        \end{equation*}
        Therefore, the graded sympletic action is $(\Omega^{-1})_{ij} = i \delta_{ij}$ or $\Omega^{ij} = -i \delta^{ij}$ and the canonical quantised anticommutator is 
        \begin{equation*}
            \{\hat \psi^i, \hat \psi^j\} = i \hbar (-i \delta^{ij}) = \hbar \delta^{ij} ~.
        \end{equation*}
        Notice that, if we define $\hat \psi^i = \sqrt{\frac{\hbar}{2}} \gamma^i$, we recover the Clifford algebra $\{\gamma^i, \gamma^j\} = 2 \delta^{ij}$. We need to find the irreducible representation of this algebra to see the space in which states live. The dimension is
        \begin{equation*}
            \dim \mathcal F = 2^{[n/2]} \times 2^{[n/2]} ~,
        \end{equation*}
        where $[n/2]$ is the integer part. The Fock space has dimension $2$ if $n=2,3$, dimension $4$ if $n = 4,5$, etc.
    \end{example}

    \begin{example}
        Consider a complex Grassmann variables $\psi$ and $\psi^*$. The action is 
        \begin{equation*}
            S[\psi, \psi^*] = \int dt (i \psi^* \dot \psi - H(\psi, \psi^*)) ~.
        \end{equation*}
        Therefore, the graded sympletic action is $\Omega^{ij} = -i$ and the canonical quantised anticommutator is 
        \begin{equation*}
            \{\hat \psi, \hat \psi^\dagger \} = i \hbar (-i) = \hbar ~.
        \end{equation*}
        Hence, we have recovered the fermionic harmonic oscillator and with the same procedure of the Fock construction, we can build the Fock space.
    \end{example}

    With these examples, we can build irreducible representation for the gamma matrices in arbitrary dimensions. In even dimension $n = 2m$, we combine $2m$ Maiorana fermions into $m$ Dirac fermions, which generates a set of $m$ independent anticommuting ladder operators, acting on the tensor product of $m$ $2$-dimensional fermionic Fock spaces, which is a total space of dimension $2^m$, compatible with the dimensionality of the gamma matrices. Adding an extra Maiorana fermion $n + 1$, the dimensionality does not change since they are proportional to the chiral operator which already is in the $n$ space.

\section{Coherent states}

    The last step for fermionic path integral is to introduce a change of basis for the Hilbert space, using overcompleted coherent states.

    In the bosonic case, the harmonic oscillator has ladder operators defined by the commutation relations
    \begin{equation}\label{f:comm}
        [\hat a, \hat a^\dagger] = 1 ~, \quad [\hat a, \hat a] = [\hat a^\dagger, \hat a^\dagger] = 0 ~.
    \end{equation}
    The infinite-dimensional Fock space can be expressed by an orthonormal basis $\ket{0}, \ket{1}, \ldots \ket{n}, \ldots$ such that
    \begin{equation*}
        \hat a \ket{0} = 0 ~, \quad \hat a^\dagger \ket{0} = \ket{1} ~, \quad \ldots ~, \quad \frac{(\hat a^\dagger)^n}{n!} \ket{0} = \ket{n} ~, \quad \ldots ~,
    \end{equation*}
    such that $\braket{m}{n} = \delta_{mn}$ with $m, n = 0, 1, 2, \ldots$.
    However, we can use a new basis formed by the  eigenstates of the annihilation operator 
    \begin{equation*}
        \hat a \ket{a} = a \ket{a} ~, \quad \bra{a} \hat a^\dagger = \bra{a} a ~,
    \end{equation*}
    where $a \in \mathbb C$. An explicit realisation can be 
    \begin{equation*}
        \ket{a} = \exp(a \hat a^\dagger) \ket{0} ~, \quad \bra{a^*} = \bra{0} \exp(a^* \hat a)
    \end{equation*}
    \begin{proof}
        In fact 
        \begin{equation*}
        \begin{aligned}
            \ket{a} & = \exp(a \hat a^\dagger) \ket{0} = (1 + a \hat a^\dagger + \ldots + \frac{(a \hat a^\dagger)^n}{n!} + \ldots) \ket{0} \\ & =  \ket{0} + a \hat a^\dagger \ket{0} + \ldots + \frac{(a \hat a^\dagger)^n}{n!} \ket{0} + \ldots = (\ket{0} + a \ket{1} + \ldots + a^n \ket{n} + \ldots)  ~,
        \end{aligned}
        \end{equation*}
        hence 
        \begin{equation*}
        \begin{aligned}
            \hat a \ket{a} & = \hat a (\ket{0} + a \ket{1} + \ldots + a^n \ket{n} + \ldots) = 0 + a \ket{0} + \ldots + \frac{a^n}{\sqrt{(n-1)!}} \ket{n-1} + \ldots \\ & = a (\ket{0} + \ldots + \frac{a^{n-1}}{\sqrt{(n-1)!}} \ket{n-1} + \ldots) = a \ket{a} ~.
        \end{aligned}
        \end{equation*}
    \end{proof}
    A possible realisation of the algebra~\eqref{f:comm} is 
    \begin{equation*}
        a^* \rightarrow \hat a^\dagger  ~, \quad \pdv{}{a^*} \rightarrow \hat a ~.
    \end{equation*}
    Therefore, we have for $\ket{a} = \exp(a a^*)$
    \begin{equation*}
        \hat a \ket{a} = \pdv{}{a^*} \exp(a a^*) = a \exp(a a^*) = a \ket{a} ~.
    \end{equation*}
    They satisfy the following properties
    \begin{enumerate}
        \item scalar product, i.e.
            \begin{equation*}
                \braket{a^*}{a} = \exp(a^* a) ~,
            \end{equation*}
        \item resolution of the identity, i.e. 
            \begin{equation*}
                \int \frac{da^* da}{2 \pi i} \exp(- a^* a) \ket{a} \bra{a^*} = \mathbb I ~,
            \end{equation*}
        \item trace, i.e. 
            \begin{equation*}
                \tr \hat A = \int \frac{da^* da}{2 \pi i} \bra{a^*} \hat A \ket{a} ~.
            \end{equation*}
    \end{enumerate}
    \begin{proof}
        For the first, 
        \begin{equation*}
        \begin{aligned}
            \braket{a^*}{a} & = \bra{0} \exp(a^* \hat a) \exp(a \hat a^\dagger) \ket{0} = \bra{0} (1 + a^* \hat a + \ldots) (1 + a \hat a^\dagger + \ldots) \ket{0} \\ & =  \bra{0} (1 + a^* \hat a + a \hat a^\dagger + a^* a \hat a \hat a^\dagger + \ldots ) \ket{0} \\ & = \underbrace{\braket{0}{0}}_1 + a^* \bra{0} \underbrace{\hat a \ket{0}}_0 + a \underbrace{\bra{0} \hat a^\dagger}_0 \ket{0} + a^* a \underbrace{\bra{0} \hat a}_{\bra{1}} \underbrace{\hat a^\dagger \ket{0}}_{\ket{1}} + \ldots \\ & = 1 + a^* a \underbrace{\braket{1}{1}}_1 + \ldots = 1 + a^* a + \ldots = \exp(a^* a) ~.
        \end{aligned}
        \end{equation*}
    \end{proof}
    Notice that coherent states are an over-complete basis, in fact they are not orthonormal $\braket{b^*}{a} = \exp(b^* a) \neq 0$.

    The same procedure can be used for fermionic harmonic oscillator, which has ladder operators satisfying the anti-commutator relations
    \begin{equation*}
        \{\hat \psi, \hat \psi^\dagger \} = 1 ~, \quad \{\hat \psi, \hat \psi\} = \{\hat \psi^\dagger, \hat \psi^\dagger\} = 0 ~.
    \end{equation*}
    which is realised by $2 \times 2$ matrices acting on a $2$-dimensional fermionic Fock space, spanned by $\ket{0}$ and $\ket{1}$ such that 
    \begin{equation*}
        \hat \psi \ket{0} = 0 ~, \quad \hat \psi^\dagger \ket{0} = \ket{1} ~.
    \end{equation*}
    The coherent states are defined as eigenstates of the annihilation operator 
    \begin{equation*}
        \hat \psi \ket{\psi} = \psi \ket{\psi} ~, \quad \bra{\psi^*} \hat \psi^\dagger = \bra{\psi^*} \psi^* ~,
    \end{equation*}
    where $\psi$ is a complex Grassmann variable. An explicit realisation can be 
    \begin{equation*}
        \ket{\psi} = \exp(\hat \psi^\dagger \psi) \ket{0} ~, \quad \bra{\psi^*} = \bra{0} \exp(\psi^* \hat \psi)
    \end{equation*}
    \begin{proof}
        In fact 
        \begin{equation*}
            \ket{\psi} = \exp(\hat \psi^\dagger \psi) \ket{0} = (1 + \hat \psi^\dagger \psi) \ket{0} = \ket{0} - \psi \ket{1} ~,
        \end{equation*}
        hence 
        \begin{equation*}
            \hat \psi \ket{\psi} = \hat \psi (\ket{0} - \psi \ket{1}) = \psi \hat \psi \ket{1} = \psi \ket{0} = \psi (\ket{0} - \psi \ket{1}) = \psi \ket{\psi} ~,
        \end{equation*}
        where we have used the fact that $\psi^2 = 0$.
    \end{proof}
    They satisfy the following properties
    \begin{enumerate}
        \item scalar product
            \begin{equation*}
                \braket{\psi^*}{\psi} = \exp(\psi^* \psi) ~,
            \end{equation*}
        \item resolution of the identity, i.e. 
            \begin{equation*}
                \int d\psi^* d\psi \exp(- \psi^* \psi) \ket{\psi} \bra{\psi^*} = \mathbb I ~,
            \end{equation*}
        \item trace of a bosonic operator $\hat A$, i.e. 
            \begin{equation*}
                \tr \hat A = \int d\psi^* d\psi \bra{-\psi^*} \hat A \ket{\psi} ~.
            \end{equation*}
        \item supertrace, i.e. 
            \begin{equation*}
                \str \hat A = \int d\psi^* d\psi \bra{\psi^*} \hat A \ket{\psi} = \tr \Big ((-1)^{\hat \psi^\dagger \hat \psi} \hat A \Big ) ~.
            \end{equation*}
    \end{enumerate}
    \begin{proof}
        For the first, 
        \begin{equation*}
        \begin{aligned}
            \braket{\psi^*}{\psi} & = \bra{0} \exp(\psi^* \hat \psi) \exp(\hat \psi^\dagger \psi) \ket{0} = \bra{0} (1 + \psi^* \hat \psi) (1 + \hat \psi^\dagger \psi) \ket{0} \\ & = \underbrace{\braket{0}{0}}_1 + \bra{0} \psi^* \underbrace{\hat \psi \ket{0}}_0 + \underbrace{\bra{0} \hat \psi^\dagger}_0 \psi \ket{0} + \bra{0} \psi^* \hat \psi \psi^\dagger \psi \ket{0} \\ & = 1 + \underbrace{\bra{0} \hat \psi }_{\bra{1}} \psi^* \psi \underbrace{\psi^\dagger \ket{0}}_{\ket{1}} = 1 + \psi^* \psi \braket{1}{1} = 1 + \psi^* \psi = \exp(\psi^* \psi) ~.
        \end{aligned}
        \end{equation*}
        For the second,
        \begin{equation*}
        \begin{aligned}
            & \int d\psi^* d \psi \exp(- \psi^* \psi) \ket{\psi} \bra{\psi^*} = \int d\psi^* d \psi \exp(- \psi^* \psi) \exp(\hat \psi^\dagger \psi) \ket{0} \bra{0} \exp( \psi^* \hat \psi) \\ & = \int d\psi^* d\psi (1 - \psi^* \psi) (1 + \hat \psi^\dagger \psi) \ket{0} \bra{0} (1 + \psi^* \hat \psi) \\ & = \int d\psi^* d\psi (\ket{0} \bra{0} + \psi^\dagger \psi \ket{0} \bra{0} + \ket{0} \bra{0} \psi^* \hat \psi + \hat \psi^\dagger \psi \ket{0} \bra{0}  \psi^* \hat \psi \\ & \quad - \psi^* \psi \psi^\dagger \psi \ket{0} \bra{0} - \psi^* \psi \ket{0} \bra{0} \psi^* \hat \psi - \psi^* \psi \hat \psi^\dagger \psi \ket{0} \bra{0}  \psi^* \hat \psi) \\ & =  \pdv{}{\psi^*} \pdv{}{\psi} (\ket{0} \bra{0} - \psi \ket{1} \bra{0} - \ket{0} \bra{1} \psi^* - \hat \psi \ket{1} \bra{1} \psi^* \\ & \quad + \psi^* \psi \psi^\dagger \psi \ket{1} \bra{0} + \psi^* \psi \ket{0} \bra{1} \psi^* - \psi^* \psi \psi \ket{1} \bra{1} \psi^*) \\ & = \ket{0} \bra{0} + \ket{1} \bra{1} = \mathbb I ~.
        \end{aligned}
        \end{equation*}
        For the third, 
        \begin{equation*}
        \begin{aligned}
            & \int d\psi^* d\psi \exp(-\psi^* \psi) \bra{-\psi^*}  \hat A \ket{\psi} \\ & = \int d\psi^* d\psi \exp(-\psi^* \psi)  \bra{0} \exp(- \psi^* \hat \psi) \hat A \exp(\hat \psi^\dagger \psi) \ket{0} \\ & = \int d\psi^* d\psi (1 - \psi^* \psi) \bra{0} (1 - \psi^* \hat \psi) \hat A (1 + \hat \psi^\dagger \psi) \ket{0} \\ & = \int d\psi^* d\psi (\bra{0} \hat A \ket{0} - \bra{0} \psi^* \hat \psi \hat A \ket{0} + \bra{0} \hat A \hat \psi^\dagger \psi \ket{0} - \bra{0} \psi^* \hat \psi \hat A \hat \psi^\dagger \psi \ket{0}) \\ & = \int d\psi^* d\psi (\bra{0} \hat A \ket{0} - \bra{0} \psi^* \hat A \underbrace{\hat \psi \ket{0}}_0 + \underbrace{\bra{0} \hat \psi^\dagger}_0 \hat A \psi \ket{0} - \underbrace{\bra{0} \hat \psi}_{\bra{1}} \psi^* \hat A \psi \underbrace{\hat \psi^\dagger \ket{0}}_{\ket{1}}) \\ & = \pdv{}{\psi^*} \pdv{}{\psi} (\bra{0} \hat A \ket{0} - \psi^* \psi \bra{1} \hat A \ket{1}) \\ & = \pdv{}{\psi^*} (\bra{0} \hat A \ket{0} + \psi^* \bra{1} \hat A \ket{1}) = \bra{0} \hat A \ket{0} + \bra{1} \hat A \ket{1} = \tr \hat A ~.
        \end{aligned}
        \end{equation*}
        For the fourth, 
        \begin{equation*}
        \begin{aligned}
            & \int d\psi^* d\psi \exp(-\psi^* \psi) \bra{\psi^*}  \hat A \ket{\psi} \\ & = \int d\psi^* d\psi \exp(-\psi^* \psi)  \bra{0} \exp(\psi^* \hat \psi) \hat A \exp(\hat \psi^\dagger \psi) \ket{0} \\ & = \int d\psi^* d\psi (1 - \psi^* \psi) \bra{0} (1 + \psi^* \hat \psi) \hat A (1 + \hat \psi^\dagger \psi) \ket{0} \\ & = \int d\psi^* d\psi (\bra{0} \hat A \ket{0} + \bra{0} \psi^* \hat \psi \hat A \ket{0} + \bra{0} \hat A \hat \psi^\dagger \psi \ket{0} + \bra{0} \psi^* \hat \psi \hat A \hat \psi^\dagger \psi \ket{0}) \\ & = \int d\psi^* d\psi (\bra{0} \hat A \ket{0} + \bra{0} \psi^* \hat A \underbrace{\hat \psi \ket{0}}_0 + \underbrace{\bra{0} \hat \psi^\dagger}_0 \hat A \psi \ket{0} + \underbrace{\bra{0} \hat \psi}_{\bra{1}} \psi^* \hat A \psi \underbrace{\hat \psi^\dagger \ket{0}}_{\ket{1}}) \\ & = \pdv{}{\psi^*} \pdv{}{\psi} (\bra{0} \hat A \ket{0} + \psi^* \psi \bra{1} \hat A \ket{1}) \\ & = \pdv{}{\psi^*} (\bra{0} \hat A \ket{0} - \psi^* \bra{1} \hat A \ket{1}) = \bra{0} \hat A \ket{0} - \bra{1} \hat A \ket{1} = \str \hat A ~.
        \end{aligned}
        \end{equation*}
    \end{proof}

\chapter{Fermionic path integral}

    In this chapter, we will study fermionic path integrals.

\section{Path integral}

    Consider a normal ordered hamiltonian $\hat H(\hat \psi, \hat \psi^\dagger)$, which can be $\hat H = \omega_0 + \omega \hat \psi^\dagger \hat \psi$. The transition amplitude between the initial state $\ket{\psi_i}$ and the final state $\bra{\psi^*_f}$ is 
    \begin{equation*}
    \begin{aligned}
        \bra{\psi^*_f} \exp(- i \hat H T) \ket{\psi_i} & = \bra{\psi^*_f} \exp(- i \hat H \frac{T}{N} N) \ket{\psi_i} \\ & = \bra{\psi^*_f} \exp(- i \hat H \epsilon) \ldots \exp(- i \hat H \epsilon) \ket{\psi_i} \\ & = \bra{\psi^*_f} \exp(- i \hat H \epsilon) \mathbb I \ldots \mathbb I \exp(- i \hat H \epsilon) \ket{\psi_i} \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \bra{\psi_k^*} \exp(- i \hat H \epsilon) \ket{\psi_{k-1}} \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \bra{\psi_k^*} (1 - i \epsilon \hat H + \ldots) \ket{\psi_{k-1}} \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \braket{\psi_k^*}{\psi_{k-1}} (1 - i \epsilon H (\psi_k^*,\psi_{k-1}) + \ldots) \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \exp(\psi^*_k \psi_{k-1}) \exp(- i \epsilon H (\psi^*_k, \psi_{k-1})) ~,
    \end{aligned}
    \end{equation*}
    hence 
    \begin{equation*}
    \begin{aligned}
        & \lim_{N \rightarrow \infty} \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \exp(\psi^*_k \psi_{k-1}) \exp(- i \epsilon H (\psi^*_k, \psi_{k-1})) \\ & = \lim_{N \rightarrow \infty} \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \exp( \sum_{k=1}^{N} psi^*_k \psi_{k-1} - i \epsilon H (\psi^*_k, \psi_{k-1}) + \psi^*_k \psi_k - \psi^*_k \psi_k) \\ & = \lim_{N \rightarrow \infty} \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \exp( \sum_{k=1}^{N} - psi^*_k \frac{\psi_k - \psi_{k-1}}{\epsilon} - i \epsilon H (\psi^*_k, \psi_{k-1}) + \psi^*_N \psi_N) \\ & = \int \mathcal D \psi^* \mathcal D \psi \exp(i \int dt (i \psi^* \dot \psi - H(\psi^*, \psi)) + \psi^*(T) \psi(T)) \\ & = \int \mathcal D \psi^* \mathcal D \psi \exp(i S[\psi^*, \psi]) ~,
    \end{aligned}
    \end{equation*}
    where the action is 
    \begin{equation*}
        S[\psi, \psi^*] = \int_0^T dt (i \psi^* \dot \psi - H(\psi, \psi^*)) - i \psi^*(T) \psi(T) ~.
    \end{equation*}
    The last term is there to ensure the only allowed boundary conditions $\psi(0) = \psi_i$ and $\psi^*(T) = \psi_f$.
