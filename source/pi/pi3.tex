\part{Fermionic path integral}

\chapter{Grassmann or anticommuting numbers}

    An $n$-dimensional Grassman algebra $\mathfrak G_n$ is generate by a set of generators $\theta_i$, called Grassmann variables or anticommuting numbers, such that they satisfy
    \begin{equation*}
        \{\theta_i, \theta_j\} = 0 ~, \quad \theta_i \theta_i = 0~.
    \end{equation*}

\section{Operation with Grassmann variables} 

\subsection{Functions}

    A function of the Grassmann variables can be defined by its Taylor expansion. For $n = 1$, we have only $\theta$ such that $\theta^2 = 0$ and the function $f$ can be expressed as
    \begin{equation*}
        f(\theta) = f_0 + f_1 \theta ~,
    \end{equation*}
    in which we cannot have more terms since $\theta^2 = 0$ cancels them out. For $n=2$, we have $\theta_1, \theta_2$ such that $\theta_1 \theta_2 = -\theta_2 \theta_1$, $\theta_1^2 = \theta_2^2 = 0$ and the function $f$ can be expressed as
    \begin{equation*}
        f(\theta_1, \theta_2) = f_0 + f_1 \theta_1 + f_2 \theta_2 + f_3 \theta_1 \theta_2 ~.
    \end{equation*} 
    where the term $\theta_2 \theta_1$ is not written since it is not independent by the anticommutation relation. We should have written something like 
    \begin{equation*}
        a \theta_1 \theta_2 + b \theta_2 \theta_1 = a \theta_1 \theta_2 + b \theta_2 \theta_1 = \underbrace{(a - b)}_{f_3} \theta_1 \theta_2 = f_3 \theta_1 \theta_2 ~.
    \end{equation*}
    An example of function is the exponential 
    \begin{equation*}
        \exp(\theta) = 1 + \theta ~,
    \end{equation*}
    where $f_0 = 1$ and $f_1 = 1$. Terms with an even number of Grassmann variables are called Grassmann even and terms with an odd number of Grassmann variables are called Grassmann odd.

\subsection{Derivatives}

    Derivatives are simple since all the functions are linear in Grassmann variables. However, we can define $2$ kind of derivatives: left and right. The only difference, by the anticommutativity relation, is only a minus sign. For example in $n = 2$, we have 
    \begin{equation*}
        \pdv{_l}{\theta_1} f(\theta_1, \theta_2) = \pdv{_l}{\theta_1} (f_0 + f_1 \theta_1 + f_2 \theta_2 + f_3 \theta_1 \theta_2) = f_1 + f_3 \theta_2 ~,
    \end{equation*}
    \begin{equation*}
        \pdv{_r}{\theta_1} f(\theta_1, \theta_2) = \pdv{_r}{\theta_1} (f_0 + f_1 \theta_1 + f_2 \theta_2 + f_3 \theta_1 \theta_2) = \pdv{_r}{\theta_1} (f_0 + f_1 \theta_1 + f_2 \theta_2 - f_3 \theta_2 \theta_1) = f_1 - f_3 \theta_2 ~.
    \end{equation*}
    In these notes, we will use always left derivatives.

\subsection{Integrals}

    Berezin integrals are defined to be identical with derivatives
    \begin{equation*}
        \int d\theta = \pdv{}{\theta} ~.
    \end{equation*}

    It is translational invariant
    \begin{equation*}
        \int d\theta f(\theta + \eta) = \int d\theta f(\theta) ~,
    \end{equation*}
    where $\theta, \eta \in \mathfrak G$.
    \begin{proof}
        In fact 
        \begin{equation*}
            \int d\theta f(\theta + \eta) = \pdv{}{\theta} f(\theta + \eta) = \pdv{}{\theta} (f_0 + f_1 (\theta + \eta)) = f_1 = \int d\theta f(\theta) ~.
        \end{equation*}
    \end{proof}

    As a consequence, we have 
    \begin{equation*}
        \int d\theta f(\theta + \eta) = \int d(\theta + \eta) f(\theta + \eta) = \int d\theta f(\theta) ~.
    \end{equation*}

\subsection{Reality properties}

    A Grassmann variable can be either real, i.e. $\theta = \theta^*$, or complex, i.e. $\theta \neq \theta^*$. The product of two conjugate Grassmann variables is 
    \begin{equation*}
        (\theta_1 \theta_2)^* = \theta_2^* \theta_1^* ~.
    \end{equation*}

    The complex conjugate of the product of $2$ real Grassmann variables $\theta_1^* = \theta_1$ and $\theta_2^* = \theta_2$ is purely imaginary 
    \begin{equation*}
        (\theta_2 \theta_1)^* = \theta_1^* \theta_2^* = \theta_1 \theta_2 = - \theta_2 \theta_1 ~,
    \end{equation*}
    whereas if we multiply by $i$ we obtain a real Grassmann variable 
    \begin{equation*}
        (i \theta_2 \theta_1)^* = - i\theta_1^* \theta_2^* = - i \theta_1 \theta_2 = i \theta_2 \theta_1 ~.
    \end{equation*}

    We can always express complex Grassmann variables $\eta$ in terms of real ones $\theta_1$ and $\theta_2$
    \begin{equation*}
        \eta = \frac{\theta_1 + i \theta_2}{\sqrt 2} ~, \quad \eta^* = \frac{\theta_1 - i \theta_2}{\sqrt 2} ~.
    \end{equation*}

\subsection{Gaussian integration}

    For a single Grassmann variable $\theta$, the Gaussian integral is trivial, since $\theta^2 = 0$ and 
    \begin{equation*}
        \int d\theta \exp(a \theta^2) = \int d\theta 1 =  \pdv{}{\theta} 1 = 0 ~.
    \end{equation*}
    We need at least $2$ Grassmann variables $\theta_1$ and $\theta_2$ to have a non trivial Gaussian integral 
    \begin{equation*}
        \int d\theta_1 d \theta_2 \exp(- a \theta_1 \theta_2) = \pdv{}{\theta_1} \pdv{}{\theta_2} (1 - a \theta_1 \theta_2) = \pdv{}{\theta_1} \pdv{}{\theta_2} (1 + a \theta_2 \theta_1 ) = \pdv{}{\theta_1} (a \theta_1) = a ~.
    \end{equation*}
    We can rewritten this integral in terms of a skew-symmetric matrix $A^{ij}$ defined as 
    \begin{equation*}
        A = \begin{bmatrix}
            0 & a \\ -a & 0 \\
        \end{bmatrix} ~,
    \end{equation*}
    so that it becomes 
    \begin{equation*}
        \int d\theta_1 d \theta_2 \exp(- \frac{1}{2} \theta_i A^{ij} \theta_j) = \det^{1/2} A^{ij} ~,
    \end{equation*}
    where in our case $\det A = a^2 \geq 0$. Notice that in the bosonic case, we had $\det^{- 1/2}$. Generalising for an even number of Grassmann variables $n = 2$, we have 
    \begin{equation*}
        \int d^n \theta \exp(- \frac{1}{2} \theta_i A^{ij} \theta_j) = \det^{1/2} A^{ij} ~,
    \end{equation*}
    where $d^n \theta = d\theta_1 \ldots d\theta_n$ and $A$ is a skew-symmetric block-diagonal matrix 
    \begin{equation*}
        A = \begin{bmatrix}
            0 & -a_1 & 0 & \ldots & 0 \\
            - a_1 & 0 & 0 & \ldots & \ldots \\
            0 &  0 & \ldots & 0 & 0 \\
            \ldots &  \ldots & 0 & 0 & a_n \\
            0 &  \ldots & 0 & - a_n & 0 \\
        \end{bmatrix} ~.
    \end{equation*}
    whose determinant is $\det^{1/2} A = a_1 \ldots a_n$. The skew-diagonalisation can be made by an orthogonal matrix. For complex variables $\eta_i$, we have 
    \begin{equation*}
        \int d^n \eta^*  \int d^n \eta \exp(- \frac{1}{2} \eta_i^* A^{ij} \eta_j) = \det A^{ij} ~,
    \end{equation*}
    where $\int d^n \eta^* \int d^n \eta = d \eta_1^* d \eta_1 \ldots d \eta_n^* d \eta_n$.

\chapter{Canonical quantisation} 

    In order to find physical application, we consider an infinite-dimensional Grassman algebra $\mathfrak G_\infty$, in hypercondensed notation $\theta_i \rightarrow \theta (t)$. 

\section{Fermionic harmonic oscillator}

    Consider an harmonic oscillator with mass $m = 1$ and frequency $\omega$. Its action is 
    \begin{equation*}
        S[x, p] = \int dt (p \dot x - \frac{1}{2} (p^2 + m^2 x^2)) ~.
    \end{equation*}
    We can make a change of basis, using the ladder operators 
    \begin{equation*}
        a = \frac{\omega x + i p}{\sqrt{2 \omega}} ~, \quad a^* = \frac{\omega x - i p}{\sqrt{2 \omega}} ~, 
    \end{equation*}
    hence 
    \begin{equation*}
        S[a, a^*] = \int dt (i a^* \dot a - \omega a^* a) + \textnormal{total derivatives} ~.
    \end{equation*}
    We promote it to operators $\hat a$ and $\hat a^\dagger$ via the commutation relations $[\hat a, \hat a^\dagger] = 1$ living in a Fock space. 

    Now, we consider a fermionic harmonic oscillator, described by complex Grassmann variables $\psi(t)$ and $\psi^*(t)$, with a similar action 
    \begin{equation*}
        S[\psi, \psi^*] = \int dt (i \psi^* \dot \psi - \omega \psi^* \psi)  ~.
    \end{equation*}
    The equation of motion is 
    \begin{equation*}
        i \dot \psi - \omega \psi = 0 ~.
    \end{equation*}
    Notice that it is similar to the time components of the Dirac equation, with $m = \omega$ and $\gamma^0 = - i$.

    Now, we use the canonical quantisation method in phase space. The conjugate momentum is 
    \begin{equation*}
        \pi = \pdv{L}{\psi} = - i \psi^* ~.
    \end{equation*}
    The Poisson brackets are 
    \begin{equation*}
        \{\pi, \psi\}_{PB} = - 1 ~, \quad \{\pi, \psi\}_{PB} = - i ~.
    \end{equation*}
    In the language of second quantisation, we have  
    \begin{equation*}
        \{\hat \psi, \hat \psi^\dagger\} = 1 ~, \quad \{\hat \psi, \hat \psi\} = \{\hat \psi^\dagger, \hat \psi^\dagger\} = 0 ~.
    \end{equation*}

    By the Fock construction, we can build the space in which the states live, which is the irreducible representation for the Grassmann algebra. The vacuum $\ket{0}$ is the state such that 
    \begin{equation*}
        \hat \psi \ket{0} = \bra{0} \hat \psi^\dagger = 0 ~, \quad \braket{0}{0} = 1 ~.
    \end{equation*}
    The first excited state $\ket{1}$ is the state such that
    \begin{equation*}
        \ket{1} = \hat \psi^\dagger \ket{0} ~, \quad \bra{1} = \bra{0} \hat \psi ~, \quad \braket{0}{1} = \bra{0} \hat \psi^\dagger \ket{0} = 0 ~, \quad \braket{1}{1} = \bra{0} \hat \psi \hat \psi^\dagger \ket{0} = \bra{0} \underbrace{\{\hat \psi, \hat \psi^\dagger\}}_1 \ket{0} + \bra{0} \hat \psi^\dagger \underbrace{\hat \psi \ket{0}}_0 = \braket{0}{0} = 1 ~.
    \end{equation*}
    Notice that we cannot build anymore kets since we would have 
    \begin{equation*}
        \ket{2} = \underbrace{(\hat \psi^\dagger)^2}_0 \ket{0} = 0 ~.
    \end{equation*}
    Therefore, the Fock space is 
    \begin{equation*}
        \mathcal F_2 = \spann \{\ket{0}, \ket{1} \} ~.
    \end{equation*}
    The basis is orthonormal $braket{m}{n} = \delta_{mn}$, with $m, n = 0, 1$. We can write the operators $\hat \psi$ and $\hat \psi^\dagger$ as matrices
    \begin{equation*}
        \hat \psi = \begin{bmatrix}
            \bra{0} \underbrace{\hat \psi \ket{0}}_0 & \bra{0} \underbrace{\hat \psi \ket{1}}_{\ket{0}} \\ \bra{1} \underbrace{\hat \psi \ket{0}}_0 & \bra{1} \underbrace{\hat \psi \ket{1}}_{\ket{0}} \\
        \end{bmatrix} = \begin{bmatrix}
            0 & \underbrace{\braket{0}{0}}_1 \\ 0 & \underbrace{\braket{1}{0}}_0 \\
        \end{bmatrix} = \begin{bmatrix}
            0 & 1 \\ 0 & 0 \\
        \end{bmatrix} ~, \quad \hat \psi^\dagger = \begin{bmatrix}
            \bra{0} \underbrace{\hat \psi^\dagger \ket{0}}_{\ket{1}} & \bra{0} \underbrace{\hat \psi^\dagger \ket{1}}_0 \\ \bra{1} \underbrace{\hat \psi^\dagger \ket{0}}_{\ket{1}} & \bra{1} \underbrace{\hat \psi^\dagger \ket{1}}_0 \\
        \end{bmatrix} = \begin{bmatrix}
            \underbrace{\braket{0}{1}}_0 & \\  \underbrace{\braket{1}{1}}_1 &  0 \\
        \end{bmatrix} = \begin{bmatrix}
            0 & 0 \\ 1 & 0 \\
        \end{bmatrix} ~.
    \end{equation*}
    Hence, they satisfy the anticommutation relations.
    \begin{proof}
        For the first, 
        \begin{equation*}
            \{\hat \psi, \hat \psi\} = \hat \psi \hat \psi + \hat \psi \hat \psi = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} = 0 + 0 = 0 ~.
        \end{equation*}

        For the second, 
        \begin{equation*}
            \{\hat \psi^\dagger, \hat \psi^\dagger \} = \hat \psi^\dagger \hat \psi^\dagger + \hat \psi^\dagger \hat \psi^\dagger = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 0 \\ 1 & 0\end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix}
                0 & 0 \\ 0 & 0 \\
            \end{bmatrix} ~.
        \end{equation*}

        For the third, 
        \begin{equation*}
            \{\hat \psi, \hat \psi^\dagger \} = \hat \psi \hat \psi^\dagger + \hat \psi^\dagger \hat \psi = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 0 \\ 1 & 0\end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix}
                1 & 0 \\ 0 & 1 \\
            \end{bmatrix} ~.
        \end{equation*}
    \end{proof}

\section{Hamiltonian structure of bosonic phase space}

    The hamiltonian action is written as 
    \begin{equation*}
        S[x, p] = \int dt (p \dot x - H(x, p)) ~,
    \end{equation*}
    where the first term is the sympletic one, which gives rise to the Poisson brackets. In fact, we can generally rewritten the action as 
    \begin{equation*}
        S[z] = \int dt (\frac{1}{2} z^a (\Omega^{-1})_{ab} \dot z^b - H(z)) ~,
    \end{equation*}
    where it is in first order in time since the Hamilton equations are and $\Omega_{ab}$ is an invertible antisymmetric constant-valued matrix. The latter in canonical coordinates is 
    \begin{equation*}
        \Omega_{ab} = \begin{bmatrix}
            0 & 1  \\ -1 & 0 \\
        \end{bmatrix} ~, \quad (\Omega^{-1})_{ab} = \begin{bmatrix}
            0 & -1  \\ 1 & 0 \\
        \end{bmatrix} ~.
    \end{equation*}

    Given $2$ functions $f(z)$ and $g(z)$, the Poisson brackets are defined as 
    \begin{equation*}
        \{f(z), g(z)\}_{PB} = \pdv{f}{z^a} \Omega^{ab} \pdv{g}{z^b} ~.
    \end{equation*}
    It satisfies the following properties 
    \begin{enumerate}
        \item antysymmetry, i.e.
            \begin{equation*}
                \{f, g\}_{PB} = - \{g, f\}_{PB} ~,
            \end{equation*}
        \item Leibniz rule, i.e. 
            \begin{equation*}
                \{f, g h\}_{PB} = \{f, g\}_{PB} h + g \{f, h\}_{PB}~,
            \end{equation*}
        \item Jacobi identity, i.e. 
            \begin{equation*}
                \{f, \{g, h\}_{PB} \}_{PB} + \{g, \{h, f\}_{PB} \}_{PB} + \{h, \{f, g\}_{PB} \}_{PB} ~.
            \end{equation*}
    \end{enumerate}
    Notice that $\{z^a, z^b\}_{PB} = \Omega^{ab}$.

    In canonical quantisation, we promote the Poisson brackets to commutators of operators
    \begin{equation*}
        [\hat z^a, \hat z^b] = i \hbar \Omega^{ab} ~.
    \end{equation*}
    and then you need to find the irreducible representation of this algebra to see the space in which states live.

\section{Hamiltonian structure of fermionic phase space}

    Generalising for Grassmann variables, we have 
    \begin{equation*}
        Z^A = (z^a, \theta^\alpha) ~,
    \end{equation*}
    where $z^a$ are bosonic coordinates, $\theta^\alpha$ are fermionic ones. The hamiltonian action is 
    \begin{equation*}
        S[z] = \int dt (\frac{1}{2} Z^A (\Omega^{-1})_{AB} \dot Z^B - H(Z)) ~,
    \end{equation*}
    where all terms must be Grassmann evens. The graded-sympletic matrix $\Omega$ is 
    \begin{equation*}
        \Omega^{AB} = \begin{bmatrix}
            0 & 1  \\ -1 & 0 \\
        \end{bmatrix} ~,
    \end{equation*}
    where the bosonic block is antisymmetric and the fermionic block is symmetric. 
    \begin{proof}
        If the bosonic block were symmetric, we would have a total derivative, which in the action it is a boundary term
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} x & p \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ 1 & 0
            \end{bmatrix} \begin{bmatrix} \dot x \\ \dot p \end{bmatrix} = p \dot x + \dot x p = \dv{(px)}{t} ~.
        \end{equation*}
        Instead, we have
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} x & p \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ -1 & 0
            \end{bmatrix} \begin{bmatrix} \dot x \\ \dot p \end{bmatrix} = p \dot x - \dot x p \neq \dv{(px)}{t}
        \end{equation*}

        If the fermionic block were antisymmetric, we would have a total derivative, which in the action it is a boundary term
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} \theta_1 & \theta_2 \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ -1 & 0
            \end{bmatrix} \begin{bmatrix} \dot \theta_1 \\ \dot \theta_1 \end{bmatrix} = \theta_1 \dot \theta_2 - \dot \theta_2 \theta_1 = \theta_1 \dot \theta_2 + \theta_1 \dot \theta_2 = \dv{(\theta_1 \theta_2)}{t} ~.
        \end{equation*}
        Instead, we have
        \begin{equation*}
            \frac{1}{2} \begin{bmatrix} \theta_1 & \theta_2 \end{bmatrix} \begin{bmatrix}
                0 & 1 \\ 1 & 0
            \end{bmatrix} \begin{bmatrix} \dot \theta_1 \\ \dot \theta_1 \end{bmatrix} = \theta_1 \dot \theta_2 + \dot \theta_2 \theta_1 = \theta_1 \dot \theta_2 - \theta_1 \dot \theta_2 \neq \dv{(\theta_1 \theta_2)}{t} ~.
        \end{equation*}
    \end{proof}

    Given $2$ functions $f(Z)$ and $g(Z)$, the Poisson brackets are defined as 
    \begin{equation*}
        \{f(Z), g(Z)\}_{PB} = \pdv{_r f}{Z^A} \Omega^{AB} \pdv{_l g}{Z^B} ~.
    \end{equation*}
    It satisfies the following properties 
    \begin{enumerate}
        \item graded antysymmetry, i.e.
            \begin{equation*}
                \{f, g\}_{PB} = (-1)^{\epsilon_f \epsilon_g + 1} \{g, f\}_{PB} ~,
            \end{equation*}
            where 
            \begin{equation*}
                \epsilon_f = \begin{cases}
                    0 & \textnormal{if f is bosonic} \\
                    1 & \textnormal{if f is fermionic} \\
                \end{cases}
            \end{equation*}
        \item Leibniz rule, i.e. 
            \begin{equation*}
                \{f, g h\}_{PB} = \{f, g\}_{PB} h + (-1)^{\epsilon_f \epsilon_g} g \{f, h\}_{PB}~,
            \end{equation*}
        \item Jacobi identity, i.e. 
            \begin{equation*}
                \{f, \{g, h\}_{PB} \}_{PB} + (-1)^{\epsilon_f (\epsilon_g + \epsilon_h)} \{g, \{h, f\}_{PB} \}_{PB} + (-1)^{\epsilon_h (\epsilon_f + \epsilon_g)} \{h, \{f, g\}_{PB} \}_{PB} ~.
            \end{equation*}
    \end{enumerate}
    Notice that $\{Z^A, Z^B\}_{PB} = \Omega^{AB}$.

    In canonical quantisation, we promote the Poisson brackets to graded commutators of operators
    \begin{equation*}
        [\hat Z^A, \hat Z^B\} = i \hbar \Omega^{AB} ~.
    \end{equation*}
    where the graded commutator is defined as 
    \begin{equation*}
        [\hat A, \hat B\} = \begin{cases}
            [\hat A, \hat B] & \text{if at least there is one bosonic operator} \\
            \{\hat A, \hat B\} & \text{if both are fermionic operators} \\
        \end{cases}
    \end{equation*}
    and then you need to find the irreducible representation of this algebra to see the space in which states live.

\subsection{Examples}

    Consider a single real Grassmann variable $\psi$, called also a Maiorana fermion in $0 + 1$ dimension. The action is 
    \begin{equation*}
        S[\psi] = \int dt \frac{i}{2} \psi \dot \psi ~,
    \end{equation*}
    where we have put $i$ in order to have a real action and no hamiltonian because the only most simple quadratic term vanishes $\psi^2 = 0$. Therefore, the graded sympletic action is $\Omega^{-1} = i$ or $\Omega = -i$ and the canonical quantised anticommutator is 
    \begin{equation*}
        \{\hat \psi, \hat \psi\} = i \hbar (-i) = \hbar ~.
    \end{equation*}
    Finally, the only possibility is that $\hat \psi$ is a number
    \begin{equation*}
        \hat \psi = \sqrt{\frac{\hbar}{2}} ~.
    \end{equation*}
    The Fock space is consistuted by only the vacuum state $\ket{0}$. 

    Now, consider $n$ real Grassmann variables $\psi^i$. The action is 
    \begin{equation*}
        S[\psi] = \int dt \frac{i}{2} \psi^i \delta_{ij} \dot \psi^j - H(\psi^i) ~.
    \end{equation*}
    Therefore, the graded sympletic action is $(\Omega^{-1})_{ij} = i \delta_{ij}$ or $\Omega^{ij} = -i \delta^{ij}$ and the canonical quantised anticommutator is 
    \begin{equation*}
        \{\hat \psi^i, \hat \psi^j\} = i \hbar (-i \delta^{ij}) = \hbar \delta^{ij} ~.
    \end{equation*}
    Notice that, if we define $\hat \psi = \sqrt{\frac{\hbar}{2}}$, we recover the Clifford algebra $\{\gamma^i, \gamma^j\} = 2 \delta^{ij}$. We need to find the irreducible representation of this algebra to see the space in which states live. The dimension is
    \begin{equation*}
        \dim \mathcal F = 2^{[n/2]} \times 2^{[n/2]} ~,
    \end{equation*}
    where $[n/2]$ is the integer part. The Fock space has dimension $2$ if $n=2,3$, dimension $4,5$ if $n = 4,5$, etc.

    Now, consider a complex Grassmann variables $\psi$ and $\psi^*$. The action is 
    \begin{equation*}
        S[\psi, \psi^*] = \int dt (i \psi^* \dot \psi - H(\psi, \psi^*)) ~.
    \end{equation*}
    Therefore, the graded sympletic action is $\Omega^{ij} = -i$ and the canonical quantised anticommutator is 
    \begin{equation*}
        \{\hat \psi, \hat \psi^\dagger \} = i \hbar (-i) = \hbar ~.
    \end{equation*}
    Hence, we have recovered the fermionic harmonic oscillator and wit hthe same procedure of the Fock construction, we can build the Fock space.

\chapter{Coherent states}

    It is useful to introduce a change of basis for the Hilbert space, using the coherent states. 

\section{Bosonic coherent states}

    Recall that the bosonic harmonic oscillator has ladder operators 
    \begin{equation*}
        [\hat a, \hat a^\dagger] = 1 ~, \quad [\hat a, \hat a] = [\hat a^\dagger, \hat a^\dagger] = 0 ~.
    \end{equation*}
    The infinite-dimensional Fock space can be expressed by an orthonormal diagonal basis $\ket{0}, \ket{1}, \ldots \ket{n}, \ldots$ such that
    \begin{equation*}
        \hat a \ket{0} = 0 ~, \quad \hat a^\dagger \ket{0} = \ket{1} ~, \quad \ldots ~, \quad \frac{(\hat a^\dagger)^n}{n!} \ket{0} = \ket{n} ~, \quad \ldots ~.
    \end{equation*}

    The coherent states are defined as eigenstates of the annihilation operator 
    \begin{equation*}
        \hat a \ket{a} = a \ket{a} ~, \quad \bra{a} \hat a^\dagger = \bra{a} a ~,
    \end{equation*}
    where $a \in \mathbb C$. An explicit realisation can be 
    \begin{equation*}
        \ket{a} = \exp(a \hat a^\dagger) \ket{0} ~, \quad \bra{a^*} = \bra{0} \exp(a^* \hat a)
    \end{equation*}
    \begin{proof}
        In fact 
        \begin{equation*}
        \begin{aligned}
            \ket{a} & = \ket{a} = \exp(a \hat a^\dagger) \ket{0} \\ & = (1 + a \hat a^\dagger + \ldots + \frac{(a \hat a^\dagger)^n}{n!} + \ldots) \ket{0} \\ & =  (\ket{0} + a \ket{1} + \ldots + a^n \ket{n} + \ldots)  ~,
        \end{aligned}
        \end{equation*}
        hence 
        \begin{equation*}
        \begin{aligned}
            \hat a \ket{a} = \hat a (\ket{0} + a \ket{1} + \ldots + a^n \ket{n} + \ldots) = 0 + a \ket{0} + \ldots + \frac{a^n}{\sqrt{(n-1)!}} \ket{n-1} + \ldots = a (\ket{0} + \ldots + \frac{a^{n-1}}{\sqrt{(n-1)!}} + \ldots) = a \ket{a} ~.
        \end{aligned}
        \end{equation*}
    \end{proof}

    They satisfy the following properties
    \begin{enumerate}
        \item scalar product
            \begin{equation*}
                \braket{a^*}{a} = \exp(a^* a) ~,
            \end{equation*}
        \item resolution of the identity, i.e. 
            \begin{equation*}
                \int \frac{da^* da}{2 \pi i} \exp(- a^* a) \ket{a} \bra{a^*} ~,
            \end{equation*}
        \item trace, i.e. 
            \begin{equation*}
                \tr \hat A = \int \frac{da^* da}{2 \pi i} \bra{a^*} \hat A \ket{a} ~.
            \end{equation*}
    \end{enumerate}
    \begin{proof}
        Maybe in the future.
    \end{proof}

\section{Fermionic coherent states}

    The fermionic harmonic oscillator has ladder operators 
    \begin{equation*}
        \{\hat \psi, \hat \psi^\dagger \} = 1 ~, \quad \{\hat a, \hat a\} = \{\hat \psi^\dagger, \hat \psi^\dagger\} = 0 ~.
    \end{equation*}
    The Fock space is only spanned by $\ket{0}$ and $\ket{1}$.

    The coherent states are defined as eigenstates of the annihilation operator 
    \begin{equation*}
        \hat \psi \ket{\psi} = \psi \ket{\psi} ~, \quad \bra{\psi^*} \hat \psi^\dagger = \bra{\psi^*} \psi^* ~,
    \end{equation*}
    where $\psi$ is a complex Grassmann variable. An explicit realisation can be 
    \begin{equation*}
        \ket{\psi} = \exp(\hat \psi^\dagger \psi) \ket{0} ~, \quad \bra{\psi^*} = \bra{0} \exp(\psi^* \hat \psi)
    \end{equation*}
    \begin{proof}
        In fact 
        \begin{equation*}
            \ket{\psi} = \exp(\hat \psi^\dagger \psi) \ket{0} = (1 + \hat \psi^\dagger \psi) \ket{0} = \ket{0} - \psi \ket{1} ~,
        \end{equation*}
        hence 
        \begin{equation*}
            \hat \psi \ket{\psi} = \hat \psi (\ket{0} - \psi \ket{1}) = \psi \hat \psi \ket{1} = \psi \ket{0} = \psi (\ket{0} - \psi \ket{1}) = \psi \ket{\psi} ~,
        \end{equation*}
        where we have used the fact that $\psi^2 = 0$.
    \end{proof}

    They satisfy the following properties
    \begin{enumerate}
        \item scalar product
            \begin{equation*}
                \braket{\psi^*}{\psi} = \exp(\psi^* \psi) ~,
            \end{equation*}
        \item resolution of the identity, i.e. 
            \begin{equation*}
                \int d\psi^* d\psi \exp(- \psi^* \psi) \ket{\psi} \bra{\psi^*} ~,
            \end{equation*}
        \item trace, i.e. 
            \begin{equation*}
                \tr \hat A = \int d\psi^* d\psi \bra{-\psi^*} \hat A \ket{\psi} ~.
            \end{equation*}
        \item supertrace, i.e. 
            \begin{equation*}
                \str \hat A = \int d\psi^* d\psi \bra{\psi^*} \hat A \ket{\psi} ~.
            \end{equation*}
    \end{enumerate}
    \begin{proof}
        Maybe in the future.
    \end{proof}

\chapter{Fermionic path integral}

    Consider a normal ordered hamiltonian $\hat H(\hat \psi, \hat \psi^\dagger)$, which can be $\hat H = \omega_0 + \omega \hat \psi^\dagger \hat \psi$. The transition amplitude between the initial state $\ket{\psi_i}$ and the final state $\bra{\psi^*_f}$ is 
    \begin{equation*}
    \begin{aligned}
        \bra{\psi^*_f} \exp(- i \hat H T) \ket{\psi_i} & = \bra{\psi^*_f} \exp(- i \hat H \frac{T}{N} N) \ket{\psi_i} \\ & = \bra{\psi^*_f} \exp(- i \hat H \epsilon) \ldots \exp(- i \hat H \epsilon) \ket{\psi_i} \\ & = \bra{\psi^*_f} \exp(- i \hat H \epsilon) \mathbb I \ldots \mathbb I \exp(- i \hat H \epsilon) \ket{\psi_i} \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \bra{\psi_k^*} \exp(- i \hat H \epsilon) \ket{\psi_{k-1}} \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \bra{\psi_k^*} (1 - i \epsilon \hat H + \ldots) \ket{\psi_{k-1}} \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \braket{\psi_k^*}{\psi_{k-1}} (1 - i \epsilon H (\psi_k^*,\psi_{k-1}) + \ldots) \\ & = \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \exp(\psi^*_k \psi_{k-1}) \exp(- i \epsilon H (\psi^*_k, \psi_{k-1})) ~,
    \end{aligned}
    \end{equation*}
    hence 
    \begin{equation*}
    \begin{aligned}
        & \lim_{N \rightarrow \infty} \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \prod_{k=1}^{N} \exp(\psi^*_k \psi_{k-1}) \exp(- i \epsilon H (\psi^*_k, \psi_{k-1})) \\ & = \lim_{N \rightarrow \infty} \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \exp( \sum_{k=1}^{N} psi^*_k \psi_{k-1} - i \epsilon H (\psi^*_k, \psi_{k-1}) + \psi^*_k \psi_k - \psi^*_k \psi_k) \\ & = \lim_{N \rightarrow \infty} \int ( \prod_{k=1}^{N-1} d\psi^*_k d\psi \exp(- \psi^*_k \psi_k) ) \exp( \sum_{k=1}^{N} - psi^*_k \frac{\psi_k - \psi_{k-1}}{\epsilon} - i \epsilon H (\psi^*_k, \psi_{k-1}) + \psi^*_N \psi_N) \\ & = \int \mathcal D \psi^* \mathcal D \psi \exp(i \int dt (i \psi^* \dot \psi - H(\psi^*, \psi)) + \psi^*(T) \psi(T)) \\ & = \int \mathcal D \psi^* \mathcal D \psi \exp(i S[\psi^*, \psi]) ~,
    \end{aligned}
    \end{equation*}
    where the action is 
    \begin{equation*}
        S[\psi, \psi^*] = \int_0^T dt (i \psi^* \dot \psi - H(\psi, \psi^*)) - i \psi^*(T) \psi(T) ~.
    \end{equation*}
    The last term is there to ensure the only allowed boundary conditions $\psi(0) = \psi_i$ and $\psi^*(T) = \psi_f$.