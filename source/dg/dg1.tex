\part{Manifolds}

\chapter{Manifolds and tensors}

\section{Differentiable Manifolds}

    A differential manifold $\mathcal M$ is a topological space which looks locally like $\mathbb R^N$.

    In a topological space, the notions of contiguity and continuity are well defined. A topological space $(\mathcal M, ~\{A_i\})$ is a set of points $\mathcal M$ in which is defined a family of open sets $\{A_i\}$ such that $\{ \emptyset, ~\mathcal M, ~ \cup_i A_i, ~ \cap_{i < \infty} A_i\} \in \{A_i\}$. In particular, an Haussdorf space has the property that $\forall P, ~Q \in \mathcal M \quad \exists U \in P, ~V \in Q \quad \colon \quad U \cap V = \emptyset$. Two points are contiguous if they belong to the same open subset, called neighbourhood. A map is an application $\phi \colon D \subset \mathcal M \rightarrow \mathbb R^n$. In a topological space, a map is continuous if maps open sets into open sets. 

    A chart is a pair $A, ~\phi$, where $A \subset \mathcal M$ and $\phi \colon A \rightarrow \mathbb R^n$ invertible continuous, which associates a set of $n$ real coordinates $x^i = \phi$ for the open set $A$. An atlas is a colection of charts that covers entirely the manifold $\mathcal A = \{\{(A_i ~\phi_i)\} \colon \cup_i A_i \supseteq \mathcal M \} $. A consistency map between two charts $\phi_1$ and $\phi_2$, over a point $P \in A_1 \cap A_2$, is $\phi \colon \phi(A_2) \subseteq \mathbb R^n \rightarrow \psi(A_2) \subseteq \mathbb R^n $ invertible such that $\psi(\phi_1(P)) = \phi_2(P)$ or $(\phi_2^{-1} \circ \psi \circ \phi_1) = \mathbb I$ or, equivalently, $\psi^{-1} (\phi_2(P)) = \phi_1(P)$ or $(\phi_1^{-1} \circ \psi \circ \phi_2) = \mathbb I$. $\psi$ is a change of coordinates in $\mathbb R^n$. It follows that the dimension $n$ must be the same for all charts, hence it is the dimension of the manifold. If $\psi \in C^p(\mathbb R^n)$, the manifold is a p-differentiable manifold.

    A manifold is an equivalence class of atlases, where two atlases are equivalent if there exists a bijective correspondence between them.

\section{Curves}

    A curve is a continuous map $\gamma \colon I \subseteq \mathbb R \rightarrow \mathcal M$. Introducing a chart $\phi \circ \gamma \colon I \subseteq \mathbb R \rightarrow \mathbb R^n$, or $x^i = x^i(\lambda)$, where $\lambda$ is a real parameter. If $x^i(\lambda) \in C^p(\mathbb R)$, then $gamma$ is p-differentiable. A reparameterization $\gamma' = \gamma'(\gamma)$ defines a different curve, although the images of the curves coincide.

\section{Scalars}

    A function is a map $f \colon \mathcal M \rightarrow \mathbb R$. Introducting a chart $f \circ \phi^{-1} \colon \mathbb R^n \rightarrow \mathbb R$, or $f = f(x^i)$. If $\phi'$ is another chart, then $f'(x'(P)) = f(x(P))$, showing that it is indeed a scalar. 

\section{Vectors}

    A vector at a point $P \in \mathcal M$ is a map that associates to the derivative to a function defined in a neighbourhood of $P$ $v_{\gamma} \colon f \rightarrow v_\gamma(f) = \dvin{f}{\lambda}{\lambda_P} \in \mathbb R$, where $\gamma(\lambda_P) = P$. Introducing a chart 
    \begin{equation*}
    \begin{aligned}
        v_{\gamma, ~P} (f) & = \dvin{(f \circ \gamma)}{\lambda}{\lambda_P} \\ & = \dv{}{\lambda} (f \circ \mathbb I \circ \gamma) \Big\vert_{lambda_P} \\ & = \dv{}{\lambda} (f \circ \phi^{-1} \circ \phi \circ \gamma) \Big\vert_{lambda_P} \\ & = \dv{}{\lambda} (f(x^i) \circ x^i(\lambda)) \Big\vert_{\lambda_P} \\ & = \dv{}{\lambda} f(x^i(\lambda)) \Big\vert_{\lambda_P} = \pdv{f}{x^i} \dv{x^i}{\lambda}
    \end{aligned}
    \end{equation*}
    and since it is true $\forall f$
    \begin{equation}\label{vector}
        v_\gamma = dv{}{\lambda} = \dv{x^i}{\lambda} \pdv{}{x^i}
    \end{equation}
    which means that a vector is the tangent to a curve $\gamma$ at a point $P$.

    By definition a vector is a linear functional
    \begin{equation*}
        v_\gamma (af + bg) = \dv{}{\lambda} (af+bg) = a \dv{f}{\lambda} + b \dv{g}{\lambda}
    \end{equation*}

    From~\eqref{vector}
    \begin{equation*}
        v = \underbrace{\dv{x^i}{\lambda}}_{v_i} \underbrace{\pdv{}{x^i}}_{e_i} = v^i e_i
    \end{equation*}
    where $v^i$ are the components and $e_i$ are the coordinate basis vectors, whose vectors tangent to the coordinate line defined by constant $x^j$ for $i \neq j$.

    Under a change of coordinates
    \begin{equation*}
        y^i = y^i(x^j)
    \end{equation*}
    components transform by 
    \begin{equation*}
        v^i = \dv{x^i}{\lambda} = \pdv{x^i}{y^{j'}} \dv{y^{j'}}{\lambda} = \pdv{x^i}{y^{j'}} v^{j'}
    \end{equation*}
    and basis vectors transform by 
    \begin{equation*}
        e_i = \pdv{}{x^i} = \pdv{y^{j'}}{x^i} \pdv{}{y^{j'}} = \pdv{y^{j'}}{x^i} e_{j'}
    \end{equation*}

    Remember that components transform but basis change too, inversely, then the vector remains the same.

    A vector field in an open set $U \subseteq \mathcal M$ is map from each point $P \in U$ into a vector $v(P)$. Introducing a chart, $v (x^i) = v \circ \phi^{-1}$.

    The coordinate vectors $e_i = \pdv{}{x^i}$ form a basis of a linear space composed by all the vectors tangent to a point $P$, called the tangent space $T_P$.

    \begin{proof}
    First, every tangent vector can be expressed as linear combination.
    
    Consider two curves, parametrized by $\lambda$ and $\sigma$, across a point $P$ which generate two vectors $v = \pdv{}{\lambda}$ and $w = \dv{}{\sigma}$. Hence, a generic linear combination of them
    \begin{equation*}
        a v + b w = a \dv{}{\lambda} + b \dv{}{\sigma} = a \pdv{x^i}{\lambda} \pdv{}{x^i} + b \dv{x^i}{\sigma} \pdv{}{x^i} = \Big ( a \pdv{x^i}{\lambda} + b \dv{x^i}{\sigma} \Big ) \pdv{}{x^i} = \Big ( a \pdv{x^i}{\lambda} + b \dv{x^i}{\sigma} \Big ) e_i
    \end{equation*}
    Since there are $n$ coordinates $x^i$, we have $n$ indipendent curves.

    Second, the coordinate basis vectors are linearly independent. 
    
    The determinant of the jacobian matrix of $y^i = y^i(x^j)$ must not vanish
    \begin{equation*}
        \det J = \det 
        \begin{pmatrix}
            \pdv{y^1}{x^1} & \cdots & \pdv{y^1}{x^n} \\
            \cdots & \cdots & \cdots \\
            \pdv{y^n}{x^1} & \cdots & \pdv{y^n}{x^n} \\
        \end{pmatrix}
    \end{equation*}
    hence, there are $n$ columns (or rows) which are linearly independent and also $n$ basis vector
    \begin{equation*}
        e_i = \pdv{}{x^i} = \pdv{y^j}{x^i} \pdv{}{y^j}
    \end{equation*}

    \end{proof}

    It is important to remark that coordinate basis vectors at different points belong to different tangent space and cannot be linearly combined.

\section{Fiber bundles}

    A tangent bundle is the set of all tangent space at each point together with the manifold itself $\mathcal T \mathcal M = \{\mathcal M, ~\{T_P \colon \forall P \in \mathcal M \}\}$. It can be shown that $\mathcal T \mathcal M$ is a manifold too.

\section{Exponential map}

    An integral curve $\gamma = \gamma(\lambda)$ of a vector field $V$ is the curve which as tangent vector $\dv{}{\lambda}$ has the element of $V$ in $P \in \gamma$, i.e. 
    \begin{equation*}
        V = \dv{}{\lambda}
    \end{equation*}

    Introducing a point $P_0$ and a chart $x^i$
    \begin{equation} \label{cauchy}
        \begin{split}
            V^i(\lambda) = \dv{x^i(\lambda)}{\lambda} \\
            x^i(P_0) = x^i(\lambda_0)
        \end{split}
    \end{equation}
    which are a system of $n$ Cauchy problems and the components of $V$ at an arbitrary point $ P = \phi^{-1}(x^i(\lambda))$ are $V(P) = V^i(x^j(\lambda)) \pdv{}{x^i} = V^i(\lambda) \pdv{}{x^i}$. 
    
    Theorems of calculus in $\mathbb R^n$ ensure that locally tha solution of~\eqref{cauchy} always exists, which is indeed the integral curve $\gamma(\lambda)$.

    Formally, the solution of~\eqref{cauchy} is the exponential map
    \begin{equation*}
        x^i(\lambda) = \exp((\lambda - \lambda_0)V) x^i \Big\vert_{\lambda_0}
    \end{equation*}
    which describes the flow of $V$ in a neighbourhood of $P$.

    \begin{proof}
        Let $V = \dv{}{\lambda}$ be a vector fields with integral curve $\gamma = \gamma (\lambda)$. Introducing a chart $x^i$ and Taylor expanding around $P_0$ along $\gamma$
        \begin{equation*}
        \begin{aligned}
            x^i(\lambda_0 + \epsilon) & = x^i(\lambda_0) + \epsilon \dv{x^i}{\lambda} \Big\vert_{\lambda_0} + \frac{\epsilon^2}{2} \dvd{x^i}{\lambda} \Big\vert_{\lambda_0} + \ldots \\ & = \Big (1 + \epsilon \dv{}{\lambda} \Big\vert_{\lambda_0} + \frac{\epsilon^2}{2} \dvd{}{\lambda} \Big\vert_{\lambda_0} + \ldots \Big) ~ x^i(\lambda_0) \\ & = \exp(\epsilon \dv{}{\lambda}) ~ x^i \Big\vert_{\lambda_0} \\ & = \exp(\epsilon V) ~ x^i \Big\vert_{\lambda_0} 
        \end{aligned}
        \end{equation*}
    \end{proof}
    
    For an arbitrary function $f$ in a neighbourhood of $P$
    \begin{equation*}
        f(\lambda_0 + \epsilon) = \exp \Big(\epsilon \dv{}{\lambda} \Big) ~ f \Big\vert_{\lambda_0} = \exp (\epsilon V ) ~ f \Big\vert_{\lambda_0}
    \end{equation*}

\section{Lie brackets}

    Introducing a chart $x^i$, the Lie brackets of two vector fields $V = \dv{}{\lambda} = v^i \pdv{}{x^i}$ and $W = \dv{}{\mu} = w^i \pdv{}{x^i}$ are 
    \begin{equation*}
    \begin{aligned}
        [V, ~W] & = \dv{}{\lambda} \dv{}{\mu} - \dv{}{\mu} \dv{}{\lambda} \\ & = v^i \pdv{}{x^i} \Big( w^j \pdv{}{x^j} \Big) - w^i \pdv{}{x^i} \Big( v^j \pdv{}{x^j} \Big) \\ & = \cancel{v^i w^j \pdv{}{x^i} \pdv{}{x^j}} + v^i \pdv{w^j}{x^i} \pdv{}{x^j} - \cancel{v^j w^i \pdv{}{x^i} \pdv{}{x^j}} - w^i \pdv{v^j}{x^i} \pdv{}{x^j} \\ & = \Big ( v^i \pdv{w^j}{x^i} - w^i \pdv{v^j}{x^i} \Big ) \pdv{}{x^j} 
    \end{aligned}
    \end{equation*}
    where it is used the facf that the partial derivatives commute. Hence the commutator of two vectors is still a vector. 

    The geometrical meaning of the commutator is the following: consider two vector fields $V = \dv{}{\lambda}$ and $W = \dv{}{\mu}$. Using the exponential map, the coordinates of A, moving before along $V$ and then along $W$, are 
    \begin{equation*}
        x^i(A) = \exp \Big ( \epsilon_2 \dv{}{\mu} \Big) \exp \Big ( \epsilon_1 \dv{}{\lambda} \Big) ~ x^i \Big\vert_P
    \end{equation*}
    whereas the coordinates of B, moving before along $W$ and then along $Y$, are 
    \begin{equation*}
        x^i(B) = \exp \Big ( \epsilon_1 \dv{}{\lambda} \Big) \exp \Big ( \epsilon_2 \dv{}{\mu} \Big) ~ x^i \Big\vert_P
    \end{equation*}
    Computing the difference
    \begin{equation*}
        x^i(B) - x^i(A) = \epsilon_1 \epsilon_2 \Big [\dv{}{\lambda}, ~\dv{}{\mu} \Big ] ~ x^i \Big\vert_P + O(\epsilon^3)
    \end{equation*}

    Hence, if the commutator does not vanish, the final points are different $A \neq B$ and the path $PA \cup PB$ does not close. 

    A sufficient and necessary condition for a set of fields to be coordinate vectors is that their commutator vanishes. 
    \begin{proof}
    First, the sufficient condition. Consider two coordinate vector fields $V = \pdv{}{x^1}$ and $W = \pdv{}{x^2}$. Then $v^i = \delta^i_{\phantom i 1}$, $w^j = \delta^j_{\phantom j 2}$ and the commutator vanishes, since the derivative of a constant is so. 

    Second, the necessary condition. Consider two commuting vector fields $V = \pdv{}{x^1}$ and $W = \pdv{}{x^2}$. Suppose also that they are linearly independent
    \begin{equation} \label{proof2}
        a V(P) + b W(P) = 0 \quad \iff \quad a = b = 0.
    \end{equation}
    Introducing a chart $x^i$, moving from $P$ along $V$ by $\Delta \lambda = \alpha$ to a point $R$
    \begin{equation*}
        x^(R) = \exp \Big (\alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P
    \end{equation*}
    and then along $W$ by $\Delta \mu = \beta$ to a point $Q$
    \begin{equation}\label{proof1}
        x^(Q) = \exp \Big (\beta \dv{}{\mu} \Big) \exp \Big (\alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P
    \end{equation}
    If $\alpha$ and $\beta$ are coordinates, the corresponding basis vectors are $\pdv{}{\alpha} = \pdv{x^i}{\alpha}$ and $\pdv{}{\beta} = \pdv{x^i}{\beta}$.
    Hence, using~\eqref{proof1}
    \begin{equation*}
    \begin{aligned}
        \pdv{x^i}{\alpha} & = \pdv{}{\alpha} \Big ( \exp \Big( \beta \dv{}{\mu} \Big) \exp \Big( \alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P \Big) \\ & = \exp \Big( \beta \dv{}{\mu} \Big) \pdv{}{\alpha} \Big ( \exp \Big( \alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P \Big) \\ & = \exp \Big( \beta \dv{}{\mu} \Big) \exp \Big( \alpha \dv{}{\lambda} \Big) ~ \dv{x^i}{\lambda} \Big\vert_P
    \end{aligned}
    \end{equation*}
    and, similarly, 
    \begin{equation*}
        \pdv{x^i}{\beta} = \exp \Big( \beta \dv{}{\mu} \Big) \exp \Big( \alpha \dv{}{\lambda} \Big) ~ \dv{x^i}{\mu} \Big\vert_P
    \end{equation*}
    This shows that $\pdv{}{\alpha}$ and $\pdv{}{\beta}$ are respectively the vector fields $\dv{}{\lambda}$ and $\pdv{}{\mu}$ evaluated in $Q$, using the fact that the commutator is vanishing. Then the determinant of the Jacobians is not zero
    \begin{equation*}
        \det J = \det \begin{bmatrix}
            \pdv{x^1}{\alpha} & \pdv{x^2}{\alpha} \\
            \pdv{x^1}{\beta} & \pdv{x^2}{\beta} \\
        \end{bmatrix} \neq 0
    \end{equation*}
    since we assumed the linearly independence of $\dv{}{\lambda}$ and $\dv{}{\mu}$.
    \end{proof} 

\section{1-forms}

    A 1-form is a linear functional $w$ acting on a vector $w \colon T_P \rightarrow \mathbb R$ such that $w(\alpha v + \beta u) = \alpha w(v) + \beta w(u)$ and $(\alpha w + \alpha z)(v) = \alpha w(v) + \beta z(v)$. Linearity implies that the action of a 1-form is completely determined by the action on a basis of $T_P$. 1-forms acting on the same $T_P$ form a linear space $T^*_P$, called the cotangent space. A cotangent bundle is the set of all cotangent space at each point together with the manifold itself $\mathcal T^* \mathcal M = \{\mathcal M, ~\{T^*_P \colon \forall P \in \mathcal M \}\}$. A 1-form field is a map associates a 1-form of $T^*P$ to each point $P \in \mathcal M$.

    One way of thinking 1-forms is in the following way: given an arbitrary function, a vector field is defined by $V(f) = \dv{f}{\lambda}$ whereas given an arbitrary vector field, a 1-form is defined by $V(f) = \dv{f}{\lambda} = df (\dv{}{\lambda})$. The difference is the in the former $V$ is fixed and $f$ is arbitrary, whereas in the latter $f$ is fixed and $V$ is arbitrary. Introducing a chart $x^i$
    \begin{equation*}
        \dv{f}{\lambda} = \pdv{f}{x^i} \dv{x^i}{\lambda} = \nabla_i f \dv{x^i}{\lambda} = df_i \dv{x^i}{\lambda}
    \end{equation*}
    where $df_i$ are the components of the 1-form $df$, called the gradient of $f$.
    
    Geometrically, the gradient of a function can be seen as the number of contour lines that a vector $V$ crosses in a neighbourhood of $P$. Generalizing for 1-forms, they can be seen as a set of level surfaces whose action on a vector is the number of surface the vector crosses. 

    Let $\{e_i\}$ be a basis of $T_P$. A basis of $T^*_P$ is not related to it, however it is convenient to choose the dual basis, which completely defined a basis of $T^*P$ by a basis in $T_P$ in the following way
    \begin{equation}\label{dual}
        e^i(e_j) = \delta^i_{\phantom i j}
    \end{equation}
    or, equivalently, applying it to a vector $v$
    \begin{equation*}
        e^i(v) = e^i(v^j e_j) = v^j e^i(e_j) = v^j \delta^i_{\phantom i j} = v^i
    \end{equation*}

    Consequently, $\mathcal M$, $T_P$ and $T^*_P$ have the same dimension $n$. $\{e^i\}$ are actually a basis of $T^*_P$, since given an arbitrary 1-form $q$
    \begin{equation*}
        q(v) = q(v^i e_i) = v^i q(e_i) = v^i q_i
    \end{equation*}
    and
    \begin{equation*}
        v^i q_i = q_i e^i(v)
    \end{equation*}

    Remember that under a change of coordinates, vectors or 1-forms do not change, only their components and basis change, the latter inversely.

    Differentials are the basis 1-form dual to the coordinate basis vectors 
    \begin{equation*}
        \dv{f}{\lambda} = \pdv{f}{x^i} \dv{x^i}{\lambda} = \pdv{f}{x^i} dx^i \Big(\dv{x^j}{\lambda} \pdv{}{x^j} \Big) = \pdv{f}{x^i} \dv{x^j}{\lambda} dx^i \Big ( \pdv{}{x^j} \Big) = \pdv{f}{x^i} \pdv{x^j}{\lambda} \delta^i_{\phantom i j} = \pdv{f}{x^i} \pdv{x^i}{\lambda}
    \end{equation*}
    where it has been used the dual basis.

\section{Tensors}  

    A tensor (n, m) at $P$ is a linear functional that maps $n$ 1-forms and $m$ vectors into a real number 
    \begin{equation*}
        T \colon \underbrace{T^*_P \otimes \cdots \otimes T^*_P}_{n~times} \otimes \underbrace{T_P \otimes \cdots \otimes T_P}_{m~times} \rightarrow \mathbb R
    \end{equation*}

    A tensor can be also seen as the outer product of 1-forms and vectors. A tensor (1, 0) is a vector and a tensor (0, 1) is a 1-form. A tensor (n, m) can be written in terms of the dual basis
    \begin{equation*}
        T = T^{i_1 \cdots i_n}_{j_1 \cdots j_m} e_{i_1} \otimes \cdots \otimes e_{i_n} \otimes e^{j_1} \otimes \cdots \otimes e^{j_m}
    \end{equation*}
    where the components are
    \begin{equation*}
        T^{i_1 \cdots i_n}_{j_1 \cdots j_m} = T(e^{i_1}, ~\cdots, ~e^{i_n}, ~e_{j_1}, ~\cdots, ~e^{j_m})
    \end{equation*}

    A change of basis is determined by a $4 \times 4$ non-degenerate matrix $\Lambda \in GL(n)$. On a vector basis, it acts as 
    \begin{equation}\label{proof4}
        {e'}_j = \Lambda^i_{\phantom i j} e_i
    \end{equation}
    This transformation has no effects on the dual space, however, in order to keep the duality of the basis, it must induce a transformation with the inverse matrix 
    \begin{equation*}
        {e'}^j = \Lambda^j_{\phantom j i} e^i
    \end{equation*}

    \begin{proof}
        Recalling~\eqref{dual}, to preserve the duality, also the transformed dual basis must obey 
        \begin{equation}\label{proof3}
            {e'}^i ({e'}_j) = \delta^i_{\phantom i j}
        \end{equation}
        Hence, given an arbitrary transformation matrix, 
        \begin{equation*}
            {e'}^i = M^i_{\phantom i k} e^k
        \end{equation*}
        and putting into~\eqref{proof3}, using\eqref{proof4}
        \begin{equation*}
            \delta^i_{\phantom i j} = {e'}^i ({e'}_j) = M^i_{\phantom i k} e^k (\Lambda^l_{\phantom l j} e_l) = M^i_{\phantom i k} \Lambda^l_{\phantom l j} e^k(e_l) = M^i_{\phantom i k} \Lambda^l_{\phantom l j} \delta^k_{\phantom k l} = M^i_{\phantom i k} \Lambda^k_{\phantom k j}
        \end{equation*}
        then, $M$ must satisfy
        \begin{equation*}
            M^i_{\phantom i k} \Lambda^k_{\phantom k j} = \delta^i_{\phantom j} 
        \end{equation*}
        and it is indeed the inverse matrix.
    \end{proof}

    It is possible to perform several operations on tensors at $P$:
    \begin{enumerate}
        \item scalar multiplication, i.e. 
            \begin{equation*}
                S^{(n,m)} = a T^{(n,m)} \quad \forall a \in \mathbb R
            \end{equation*}
        \item addition, i.e. 
            \begin{equation*}
                S^{(n,m)} = T^{(n,m)} + Q^{(n,m)}
            \end{equation*}
        \item outer product, i.e. 
            \begin{equation*}
                S^{(n+p,m+q)} = T^{(n,m)} \otimes Q^{(p,q)}
            \end{equation*}
        \item saturation with 1-forms, i.e. 
            \begin{equation*}
                T^{(n-1,m)} = T^{(n,m)} (\ldots, ~w, ~\ldots)
            \end{equation*}
        \item saturation with vector, i.e. 
            \begin{equation*}
                T^{(n,m-1)} = T^{(n,m)} (\ldots, ~v, ~\ldots)
            \end{equation*}
    \end{enumerate}
    The last two can be generalised to an arbitrary saturation of a $(n,m)$ tensor with a $(p<n, q<m)$ tensor. 

    For a change of basis in the tangent space to correspond a change of coordinates on the manifold, the transformation matrix must obey the condition
    \begin{equation}\label{proof8}
        \pdv{\Lambda^j_{\phantom j i}}{x^k} = \pdv{\Lambda^j_{\phantom j k}}{x^i}
    \end{equation}

    \begin{proof}
        Consider two charts $x^i$ and $y^i$ that overlap at $P$. The transformation matrix between basis is 
        \begin{equation*}
            \Lambda^i_{\phantom i j} = \pdv{x^i}{y^j}
        \end{equation*}
        and the inverse is 
        \begin{equation*}
            \Lambda^j_{\phantom j i} = \pdv{y^j}{x^i}
        \end{equation*}

        If we move continuously to another point $Q$ insider the charts, the matrix transformation will become a field $\Lambda(Q) = \Lambda(x^i(Q)) = \Lambda(y^i(Q))$ and, since the partial derivatives commute
        \begin{equation*}
            \pdv{\Lambda^j_{\phantom j i}}{x^k} = \pdv{}{x^k} pdv{y^j}{x^i} = \pdv{}{x^i} pdv{y^j}{x^k} = \pdv{\Lambda^j_{\phantom j k}}{x^i}
        \end{equation*}
    \end{proof}

\section{Metric tensor}

    The notions of lenght and angles on a manifold can be introduced with the metric tensor.

    A metric tensor $g$ is a $(2,0)$ tensor which maps two vectors into a real number, satisfying the following properties
    \begin{enumerate}
        \item symmetry, i.e.
            \begin{equation*}
                g(v,w) = g(w, v) = g(v^i e_i, w^j e_j) = g(e_i, e_j) v^i w^j = g_{ij} v^i v^j \quad \forall v, ~w \in T_P
            \end{equation*}
        \item non-degeneracy, i.i 
            \begin{equation*}
                g(v, w) = 0 \quad \forall w \in T_P \quad \iff \quad v=0 
            \end{equation*}
            or, equivalenlty, if $\det g_{ij} \neq 0$
    \end{enumerate}

    A metric tensor defines a scalar product 
    \begin{equation*}
        g(v, w) = v \cdot w
    \end{equation*}
    and introduces the notions of norm of a vector 
    \begin{equation*}
        v^2 = g(v,v) = v \cdot v = g_{ij} v^i v^j
    \end{equation*}
    and angle between two vectors 
    \begin{equation*}
        g(v, w) = v w \cos \theta
    \end{equation*}
    Although, the latter only with Riemannian metrics.

    The metric tensor, under a change of basis $\Lambda$, change 
    \begin{equation*}
        g' = \Lambda^T g \Lambda
    \end{equation*}
    where ${g'}_{ij} = g({e'}_i, {e'}_j)$. Since it is symmetric, it can be always possible to find two matrices $O^{-1} = O^{T}$ and $D = D^T = diag(\frac{1}{\sqrt{|g_{ii}^{(diag)}|}})$ such that 
    \begin{equation*}
        g' = D^T O^T g O D = D g^{(diag)} D 
    \end{equation*} 
    and put in canonical form
    \begin{equation*}
        {g'}_{ij} = \pm \delta_{ij}
    \end{equation*}
    which defines an orthonormal basis at $P$, i.e. $g(e_i, e_j) = \pm \delta_{ij}$. 

    The $\pm$ cannot be eliminated and the sum of the diagonal element is called the signature. A sign inversion does not affect the signature. The diagonal elements can classify the metric in the following way:
    \begin{enumerate}
        \item Riemannian metric, i.e. all of the same sign
        \item pseduo-Riemannian metric, i.e. both signs appear (Lorentzian metric if one is of one kind and all the others of the other kind)
    \end{enumerate}

    Metric tensors define a map between $T_P$ and $T^*_P$, to lower indices and the inverse to raise them. Infact, a vector $v \in T_P$ can be mapped into a 1-form
    \begin{equation*}
        v_i = v(e_i) = g(v^j e_j, e_i) = v^i g(e_j, e_i) = v^i g_{ij}
    \end{equation*}
    and a 1-form $w \in T^*_P$ can be mapped into a vector 
    \begin{equation*}
        w^i = e^i(w) = g(e^i, w_j e^j) = w_j g(e^i, e^j) = w_j g^{ij}
    \end{equation*}
    Consequently, at $P$ a vector and a 1-form are equivalent.

    
    The inverse metric tensor in defined by 
    \begin{equation*}
        g^{-1}_{ij} = g^{ij} \quad g_{ij} g^{jk} = \delta^k_{\phantom k i}
    \end{equation*}

    If the metric is in canonical form, the dual basis will be orthonormal. 

    A metric tensor field is a map that associates each point of $\mathcal M$ into a metric tensor. The manifold becomes a metric manifold $(\mathcal M, ~g)$. The metric tensor field in terms of coordinate vectors and dual basis is 
    \begin{equation*}
        g(x) = g_{ij}(x) dx^i \otimes dx^j
    \end{equation*}
    which is written as line element
    \begin{equation*}
        ds^2 = g_{ij}(x) dx^i dx^j
    \end{equation*}

    Consider the integral curve $\gamma$ of a vector field $v = \dv{}{\lambda}$. The scalar infinitesimal displacement along $v$ is 
    \begin{equation*}
        ds^2 = dx \cdot dx = g(dx, dx) = g(v d\lambda, v d\lambda) = g(v,v) d\lambda^2
    \end{equation*}

    Integrating along $\gamma$, the length of the path between $\lambda_1$ and $\lambda_2$ is 
    \begin{equation*}
        s(\lambda_1, \lambda_2) = \integ{\lambda_1}{\lambda_2}{\lambda} \sqrt{g(v,v)} = \integ{\lambda_1}{\lambda_2}{\lambda} \sqrt{g_{ij}(\lambda) v^i(\lambda) v^j(\lambda)}
    \end{equation*}

    Introducing a chart $x^i$, 
    \begin{equation*}
        s(\lambda_1, \lambda_2) = \integ{\lambda_1}{\lambda_2}{\lambda} \sqrt{g_{ij}(\lambda) \dv{x^i}{\lambda} \dv{x^j}{\lambda}}
    \end{equation*}


    It is always possible to find a change of coordinate that put the metric tensor field in the locally canonical form
    \begin{equation*}
        g_{ij} (x) = \pm \delta_{ij} + \frac{1}{2} \pdvd{g_{ij}}{x^k}{x^l} \Big\vert_{x_P} \delta x^k \delta x^l
    \end{equation*}
    which means to find a locally orthogonal coordinates $x^i$ such that $g(\pdv{}{x^i}, \pdv{}{x^j}) = \pm \delta_{ij}$. However, this holds only locally, not on the entire manifold.

    \begin{proof}
        Around $P$, the metric tensor field $g_{ij}$ can be Taylor expanded in $x = x_P + \delta x$
        \begin{equation} \label{proof5}
            g_{ij} = g_{ij} (x_P) + \pdv{g_{ij}}{x^k} \Big\vert_{x_P} \delta x^k + \frac{1}{2} \pdvd{g_{ij}}{x^k}{x^l} \Big\vert_{x_P} \delta x^k \delta x^l + \ldots
        \end{equation}
        as well as the transformation matrix 
        \begin{equation}\label{proof6}
            \pdv{x^i}{y^j} (x) = \pdv{x^i}{y^j} (x_P) + \pdv{}{x^k} \pdv{x^i}{y^j} \Big\vert_{x_P} \delta x^k + \frac{1}{2} \pdvd{}{x^k}{x^l} \pdv{x^i}{y^j} \Big\vert_{x_P} \delta x^k \delta x^l + \ldots
        \end{equation}
        and the metric in the new coordinates
        \begin{equation} \label{proof7}
            {g'}_{ij} = {g'}_{ij} (y_P) + \pdv{{g'}_{ij}}{y^k} \Big\vert_{y_P} \delta y^k + \frac{1}{2} \pdvd{{g'}_{ij}}{y^k}{y^l} \Big\vert_{y_P} \delta y^k \delta y^l + \ldots
        \end{equation}

        Using  
        \begin{equation*}
            {g'}_{ij} = \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl}
        \end{equation*}
        then the left-handed side is
        \begin{equation*}
        \begin{aligned}
            \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} & = 
            \Big ( \pdv{x^k}{y^i} + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a + \frac{1}{2} \pdvd{}{x^a}{x^e} \pdv{x^k}{y^i} \delta x^a \delta x^e + \ldots \Big ) \\ & \quad
            \Big ( \pdv{x^l}{y^j} + \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b + \frac{1}{2} \pdvd{}{x^b}{x^f} \pdv{x^l}{y^j} \delta x^b \delta x^f  + \ldots \Big ) \\ & \quad
            \Big ( g_{kl} + \pdv{g_{kl}}{x^c}  \delta x^c + \frac{1}{2} \pdvd{g_{kl}}{x^c}{x^d} \delta x^c \delta x^d + \ldots \Big ) 
            \\ & = 
            \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b g_{kl} 
            + \pdv{x^k}{y^i} \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b \pdv{g_{kl}}{x^c} \delta x^c 
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^c}  \delta x^c \\ & \quad 
            + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a  \pdv{x^l}{y^j} g_{kl} 
            + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b g_{kl}
            + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a  \pdv{x^l}{y^j} \pdv{g_{kl}}{x^c} \delta x^c \\ & \quad 
            + \frac{1}{2} \pdvd{}{x^a}{x^e} \pdv{x^k}{y^i} \delta x^a \delta x^e \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \frac{1}{2} \pdvd{}{x^b}{x^f} \pdv{x^l}{y^j} \delta x^b \delta x^f g_{kl} \\ & \quad 
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \frac{1}{2} \pdvd{g_{kl}}{x^c}{x^d} \delta x^c \delta x^d + \ldots 
            \\ & = \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} + \delta x^a \Big( \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} g_{kl} + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^a} + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} \Big) 
            \\ & \quad + \delta x^a \delta x^b \Big(
            \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b}
            + \pdv{}{x^a} \pdv{x^k}{y^i}\pdv{}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b}
            \\ & \qquad \qquad \qquad + \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \frac{1}{2} \pdvd{g_{kl}}{x^a}{x^b} \Big)
        \end{aligned}
        \end{equation*}

        Comparing infinitesimal of the same order
        \begin{equation*}
            \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} = {g'}_{ij}
        \end{equation*}
        \begin{equation*}
            \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} g_{kl} + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^a} + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} = \pdv{{g'}_{ij}}{y^k}
        \end{equation*}
        \begin{equation*}
        \begin{aligned}
            & \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b}
            + \pdv{}{x^a} \pdv{x^k}{y^i}\pdv{}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b} \\ & \quad + \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \frac{1}{2} \pdvd{g_{kl}}{x^a}{x^b} = \frac{1}{2} \pdvd{{g'}_{ij}}{y^k}{y^l}
        \end{aligned}
        \end{equation*}

        Looking at this system of equations, we find 1 degree of freedom for the first one, $n$ for the second one and $n^2$ for the third one. Hence, since $\Lambda$ has $n^2 - 1$ degrees of freedom with $-1$ coming from~\eqref{proof8}, we only have enough degree of freedom to put 
        \begin{equation*}
            {g'}_{ij} (y_P) = \pm \delta_{ij}
        \end{equation*}
        and 
        \begin{equation*}
            \pdv{g_{ij}}{y^k} \Big\vert_{y_P} = 0
        \end{equation*}
        but not enough to put 
        \begin{equation*}
            \pdvd{g_{ij}}{x^k}{x^l} \Big\vert_{y_P} = 0
        \end{equation*}
    \end{proof}

\chapter{Lie derivatives}

\section{Active and passive transformation}

    In the passive interpretation of a diffeomorphism on $\mathcal M$, the points remain the same but their coordinates changes. The diffeomorphism does not act on $\mathcal M$, but on the coordinates in $\mathbb R^n$. It can be sees as a change of coordinates $x' = x'(x, \epsilon)$ where $\epsilon$ is a parameter such that $x'(x, 0) = x$. For instance, a function changes in such a way that $\Phi'(x') = \Phi(x)$ where $\Phi(x) = (f \o \phi^{-1})(x)$.

    In the active interpretation of a diffeomorphism, the points are actually moved (along the flow of an integral curve). The diffeomorphism do act on $\mathcal M$. 

    The Lie dragged or push forward of a function $f$ from a point $P$ to a point $P'$ is a new function such that $f^*(P') = f(P)$. 

\section{Congruence}

    A congruence of a vector field $V$ is a set of integral curve which start from a curve $\Sigma_0$, that is an hypersurface of dimension $n-1$ and uniquely cover a portion of $\mathcal M$. A Lie dragging or a push-forward $\phi_{\Delta \lambda} \colon \mathcal M \rightarrow \mathcal M$ is the motion of a point $P(\lambda_0)$ in $P(\lambda_0 + \Delta \lambda)$ such that $\phi_{\Delta \lambda}$ is continuous and invertible. If $V \in C^{\infty}$, the push-forward becomes a diffeomorphism and form a group. Infact, $\phi_{\lambda_1} \o \phi_{\lambda_2} = \phi_{\lambda_1 + \lambda_2}$, $\phi_{\lambda}^{-1} = \phi_{- \lambda}$ and $\phi_{\lambda = 0} = \mathbb I$. 

    The push-forward of a function $f$ along a congruence of $V = \dv{}{\lambda}$ is a map $f^*$ from a point $P$ to a point $Q = \phi_{\Delta \lambda}(P) $ such that
    \begin{equation*}
        f^*_{\Delta \lambda} (Q) = f(P) 
    \end{equation*}
    If it is true $\forall Q$ along the integral curve of $V$, $f^*_{\Delta \lambda}$ is constant and $\dv{f}{\lambda} = 0$.

    The push-forward of a vector field $W = \dv{}{\mu}$ along a congruence of $V = \dv{}{\lambda}$ is a map $f^*$ from a point $P$ to a point $Q = \phi_{\Delta \lambda}(P) $ such that
    \begin{equation*}
        W^*_{\Delta \lambda} (f^*_{\Delta \lambda}) \vert_Q = W (f) \vert_P
    \end{equation*}
    where $f$ is an arbitrary function. It can be also written as 
    \begin{equation*}
        \dv{f^*}{\mu} \Big\vert_{\lambda_0 + \Delta \lambda} = \dv{f}{\mu}
    \end{equation*}
    where $\lambda(P) = \lambda_0$ and $\lambda(Q) = \lambda_0 + \Delta \lambda$.
    
    Furthermore, the commutator between $V$ and $W^*$ vanishes
    \begin{equation}\label{comm}
        [V, W^*] = [\dv{}{\lambda}, \dv{}{\mu^*}] = 0
    \end{equation}

    \begin{proof}
        Fixing $f$ and varying $\Delta \lambda$, $\dv{f^*}{\mu^*}$ is constant along the congruences of $V$. Mapping the initial curve $\Sigma_0$ into a new curve $\Sigma_{\Delta \lambda}$ and since $\lambda$ is constant, it can be used as a coordinate. 

        Since $W^*$ is tangent to $\Sigma_{\Delta \lambda}$ its parameter $\mu^*$ is constant along the congruences of $V$. Hence there are two coordinates $(\lambda, \mu^*)$ are coordinates and their coordinate vectors commute.
    \end{proof}

\section{Lie derivatives}

    The Lie derivative of a function $f$ along a vector field $V=\dv{}{\lambda}$ is 
    \begin{equation*}
        \pounds_{V} f \vert_{\lambda_0} = \lim_{\Delta \lambda \rightarrow 0} \frac{f^*_{-\Delta \lambda}(\lambda_0) - f(\lambda_0)}{\Delta \lambda} = \lim_{\Delta \lambda \rightarrow 0} \frac{f(\lambda_0 + \Delta \lambda)- f(\lambda_0)}{\Delta \lambda} = \dv{f}{\lambda} \Big \vert_{\lambda_0} = V(f)
    \end{equation*}
    where it has been used the push-back $\phi_{-\Delta\lambda} (P(\lambda_0 + \Delta \lambda)) = P(\lambda_0)$ and $f^*_{-\Delta\lambda} (\lambda_0) = f (\lambda_0 + \Delta \lambda)$. If $f$ is constant along the congruences, then $\pounds_V f = 0$.

    The Lie derivative of a vector field $W=\dv{}{\mu}$ along a vector field $V=\dv{}{\lambda}$ is
    \begin{equation*}
        \pounds_V W (f) \vert_{\lambda_0} = \lim_{\Delta \lambda \rightarrow 0} \frac{W^*_{-\Delta\lambda} - W}{\Delta \lambda} (f) \Big \vert_{\lambda_0}
    \end{equation*}
    or it can be written as 
    \begin{equation*}
        \pounds_V W = [V, W]
    \end{equation*}
    in components

    \begin{proof}
        Taylor expanding around $\lambda_0 + \Delta \lambda$ 
        \begin{equation*}
            W^*_{-\Delta\lambda} (f) \Big \vert_{\lambda_0} = \dv{f}{\mu^*} \Big \vert_{\lambda_0} = \dv{f}{\mu^*} \Big \vert_{\lambda_0 + \Delta \lambda} - \Delta \lambda \Big ( \dv{}{\lambda} \dv{}{\mu^*} f \Big)\Big \vert_{\lambda_0 + \Delta \lambda} + O(\Delta \lambda^2)
        \end{equation*}
        
        Using~\eqref{comm} and at first order $\dv{}{\mu} = \dv{}{\mu^*}$,
        \begin{equation*}
        \begin{aligned}
            \dv{f}{\mu^*} \Big \vert_{\lambda_0} & = \dv{f}{\mu} \Big \vert_{\lambda_0 + \Delta \lambda} - \Delta \lambda \Big ( \dv{}{\lambda} \dv{}{\mu^*} f \Big)\Big \vert_{\lambda_0 + \Delta \lambda} + O(\Delta \lambda^2) \\ & = \dv{f}{\mu} \Big \vert_{\lambda_0} + \Delta \lambda \dv{}{\lambda} \dv{}{\mu^*} f \Big \vert_{\lambda_0} - \Delta \lambda \Big ( \dv{}{\lambda} \dv{}{\mu^*} f \Big)\Big \vert_{\lambda_0 + \Delta \lambda} + O(\Delta \lambda^2) \\ & = \dv{f}{\mu} \Big \vert_{\lambda_0} + \Delta \lambda \Big (\dv{}{\lambda} \dv{}{\mu^*} f -  \dv{}{\mu^*} \dv{}{\lambda} f \Big) \Big \vert_{\lambda_0 + \Delta \lambda} + O(\Delta \lambda^2) \\ & = \dv{f}{\mu} \Big \vert_{\lambda_0} + \Delta \lambda \Big [\dv{}{\lambda}, \dv{}{\mu} \Big ] f \Big \vert_{\lambda_0 + \Delta \lambda} + O(\Delta \lambda^2)  
        \end{aligned}
        \end{equation*}
        Hence
        \begin{equation*}
            W^*_{-\Delta\lambda} (f) \Big \vert_{\lambda_0} = W(f) \Big \vert_{\lambda_0} + \Delta \lambda \Big [\dv{}{\lambda}, \dv{}{\mu} \Big ] f \Big \vert_{\lambda_0} + O(\Delta \lambda^2) 
        \end{equation*}
        and 
        \begin{equation*}
            \pounds_V W(f) \vert_{\lambda_0} = \lim_{\Delta \lambda \rightarrow 0} \frac{\Delta \lambda \Big [\dv{}{\lambda}, \dv{}{\mu} \Big ] f \Big \vert_{\lambda_0} (f) \Big \vert_{\lambda_0} + O(\Delta \lambda^2) }{\Delta \lambda} = \Big [\dv{}{\lambda}, \dv{}{\mu} \Big ] f \Big \vert_{\lambda_0}
        \end{equation*}
    \end{proof}

    The Lie derivative satifies the properties 
    \begin{enumerate}
        \item vanishes if the components of $W$ are constant along $V$
        \item Leibniz rule, i.e. 
            \begin{equation*}
                \pounds_V (fW) = f \pounds_V (W) + \pounds_V (f) W  
            \end{equation*}
        \item linearity, i.e.
            \begin{equation*}
                \pounds_V + \pounds_W = \pounds_{V+W}
            \end{equation*}
        \item commutator, i.e.
            \begin{equation*}
                [\pounds_V, \pounds_W] = \pounds_{[V,W]}
            \end{equation*}
        \item Jacobi identity, i.e.
            \begin{equation*}
                [[\pounds_V, \pounds_W], \pounds_Z] + [[\pounds_W, \pounds_Z], \pounds_X] + [[\pounds_Z, \pounds_V], \pounds_W] = 0 
            \end{equation*}
    \end{enumerate}

    The Lie derivative of a 1-form $\omega$ along a vector field $V=\dv{}{\lambda}$ is 
    \begin{equation*}
        (\pounds_V \omega ) (W) = \pounds_V (\omega(W)) - \omega(\pounds_V W)
    \end{equation*}
    or introducing a chart $x^i$ and the sual basis
    \begin{equation*}
        (\pounds_V \omega)_i = V^k \pdv{\omega_i}{x^k} + \omega_k \pdv{V^k}{x^i}
    \end{equation*}

    \begin{proof}
        Using the Leibniz rule,
        \begin{equation*}
            \pounds_V (\omega (W)) = (\pounds_V \omega) (W) + \omega  (\pounds_V W)
        \end{equation*}
        \begin{equation*}
             (\pounds_V \omega) (W) = \pounds_V (\omega (W)) - \omega  (\pounds_V W)
        \end{equation*}

        Introducing a coordinate basis, the Lie derivative of a scalar and the components of a commutator
        \begin{equation*}
        \begin{aligned}
            (\pounds_V \omega)_i & = (\pounds_V \omega )(e_i) \\ & = \pounds_V (\omega (e_i)) - \omega  (\pounds_V e_i) = \dv{\omega (e_i)}{\lambda} - \omega ([V, e_i]) \\ & = V^k \pdv{\omega^i}{x^k} + \omega^k \pdv{V^k}{x^i}
        \end{aligned}
        \end{equation*}
    \end{proof}

    The Lie derivative of a tensor $(n,m)$
    \begin{equation*}
        T (\omega_1, \ldots, \omega_n, W^1, \ldots, W^m) \colon \mathcal M \rightarrow \mathbb R
    \end{equation*} 
    along a vector field $V=\dv{}{\lambda}$ is
    \begin{equation*}
    \begin{aligned}
        \pounds_V T (\omega_1, \ldots, \omega_n, W^1, \ldots, W^n) & = (\pounds_V T) (\omega_1, \ldots, \omega_n, W^1, \ldots, W^n) \\ & \quad + T (\pounds_V \omega_1, \ldots, \omega_n, W^1, \ldots, W^n) + \ldots \\ & \quad + T (\pounds_V \omega_1, \ldots, \pounds_V \omega_n, W^1, \ldots, W^n) \\ & \quad + T (\omega_1, \ldots, \omega_n, \pounds_V W^1, \ldots, W^n) + \ldots \\ & \quad + T (\omega_1, \ldots, \omega_n, W^1, \ldots, \pounds_V W^n)
    \end{aligned}
    \end{equation*}

\section{Symmetries}

    Symmetries are no longer referred to the manifold, but to tensor defined in it. Furthermore, their geometrical meaning is a local feature.

    A submanifold is a subset $\mathcal S \subset \mathcal M$ of dimension $\dim \mathcal S \leq \dim \mathcal M$ sucin which there exist a chart $x^i$ such that $U \cap \mathcal S \subseteq \mathcal M$ and $x^{n-m+1} = \ldots = x^n = 0$ for all $P \in mathcal S$.

    The tangent space in a point $P \in mathcal S$ has dimension
    \begin{equation*}
        \dim T_P^{(\mathcal M)} = n \geq \dim T_P^{(\mathcal M)} = m
    \end{equation*}

    Curves and vectors in $\mathcal S$ maps to $\mathcal M$
    \begin{equation*}
        \gamma_{\mathcal S} = (x^1(\lambda), \ldots, x^m(\lambda)) \rightleftarrows \gamma_{\mathcal M} = (x^1(\lambda), \ldots, x^m(\lambda), 0, \ldots, 0)
    \end{equation*}
    and
    \begin{equation*}
        V_{\mathcal S} = (V^1, \ldots, V^m) \rightleftarrows V_{\mathcal M} = (V^1, \ldots, V^m, 0, \ldots, 0)
    \end{equation*}
    but the inverse is not unique, infact there are infinitely many curves or vectors created putting a different number from $0$ in the places with index greater than m. 

    A 1-form in the submanifold is defined as 
    \begin{equation*}
        \omega_{\mathcal S} (V) = \omega_{\mathcal M} (V, 0, \ldots, 0)
    \end{equation*}
    where $V \in T_P^{(\mathcal S)}$. Also here, the inverse is not unique, infact there are infinitely many 1-forms created putting a different number from $0$ in the places with index greater than m. 

    A set of vector fields $V^{(k)}$ with $k = 1, \ldots, p$ is linearly independent if there exist $a_k$ constants such that
    \begin{equation*}
        \sum_{k=1}^{p} a_k V^k (P) = 0 \quad \forall P \in \mathcal M 
    \end{equation*}

    This does not mean that at a given $P$, they are linearly independent, because the coefficients could depend on point $a_k = a_k (P)$.

    \begin{theorem}[Frobenius]
        Let $V^{(k)}$ be a set of linearly independent vector fields with $k = 1, \ldots, p$ such that forms a Lie algebra
        \begin{equation*}
            [V^{(i)},V^{(j)}] = C^{ij}_{\phantom{ij} k} V^{(k)}
        \end{equation*}
        where $C^{ij}_{\phantom{ij} k} \in \mathbb R$. Then the integral curves of $V^{(k)}$ form a family of submanifolds or foliations of $\mathcal M$ of dimension $m \leq p$.
    \end{theorem}

    A vector field $V$ is a symmetry of a tensor field $T$ if 
    \begin{equation*}
        \pounds_V T = 0
    \end{equation*}

    \begin{theorem}
        Let $V^{(i)}$ be a set of linearly independent vector fields with $i = 1, \ldots, p$ and $T^{(k)}$ be a set of linearly independent vector fields with $k = 1, \ldots, q$ such that 
        \begin{equation*}
            \pounds_{\sum_i a_i V^{(i)}} \sum_k b_k T^{(k)} = 0
        \end{equation*}
        Then $V^{(i)}$ form a Lie algebra
    \end{theorem}

    \begin{proof}
        Given a two symmetries $V^{(1)}$ and $V^{(2)}$, using a property of the Lie derivative
        \begin{equation*}
            [\pounds_{V^{(1)}}, \pounds_{V^{(2)}}] = \pounds_{[V^{(1)},V^{(2)}]} = 0
        \end{equation*}
        Hence, $[V^{(1)},V^{(2)}]$ is a symmetry as well. Generalizing for a linear combination $aV^{(1)} + bV^{(2)}$, the only condition to satift the hypothesis is that $a$ and $b$ are independent of $P$ and the structure constant as well. 
    \end{proof}

    \begin{corollary}
        $V^{(i)}$ define a submanifold of dimension $m \leq p$.
    \end{corollary}

    An isometries is a symmetry of the metric tensor 
    \begin{equation*}
        \pounds_V g = 0
    \end{equation*}
    where $V$ is called the Killing vector. Hence, congruences along a Killing vector preserves lengths and angles.

    In special relativity, inertial observers can be seen as coordinate frames along Killing vectors, using the Minkovski metric $g = \eta$.

\chapter{Integrals and forms}

\section{p-forms}

    A p-form is an antysymmetric tensor $(0, p)$ in the tangent $T_P$. p-forms form a linear space. 

    A 2-form $\omega$ is 
    \begin{equation*}
        \omega_{[ij]} = \frac{1}{2!} (\omega_{ij} - \omega_{ji})
    \end{equation*}

    A 3-form $\omega$ is 
    \begin{equation*}
        \omega_{[ijk]} = \frac{1}{3!} (\omega_{ijk} + \omega_{jki} + \omega_{kij} - \omega_{ikj} - \omega_{kji} - \omega_{jki})
    \end{equation*}

    A general p-form $\omega$ is 
    \begin{equation*}
        \omega_{[i_1 \ldots i_p]} = \frac{1}{p!} (\omega_{i_1 \ldots i_p} + \textnormal{permutations})
    \end{equation*}

    The number of independent components of a p-form is the binomial coefficient 
    \begin{equation*}
        \binom{n}{p} 
    \end{equation*} 
    with the condition $\sum_p \binom{n}{p} = n^2$.

    Introducing the wedge product 
    \begin{equation*}
        \omega = \omega_{i_1 \ldots i_p} e^{i_1} \wedge \ldots \wedge e^{i_p} = \omega_{i_1 \ldots i_p} \frac{1}{p!} (e^{i_1} \otimes \ldots \otimes e^{i_p} + \textnormal{permutations})
    \end{equation*}

    Moreover, the wedge product can be used to compose a p-form and a q-form into a (p+q)-form
    \begin{equation*}
        \textnormal{p-form} \wedge \textnormal{q-form} = \textnormal{(p+q)-form}
    \end{equation*}
    and to contract a p-form with a vector to obtain a (p-1)-form
    \begin{equation*}
    \begin{aligned}
        p(V, \ldots) & = (\omega_{i_1 \ldots i_p} e^{i_1} \wedge \ldots \wedge e^{i_p}) (V^k e_k) \\ & = \frac{1}{p!} (\omega_{i_1 \ldots i_p} e^{i_1}(e_k) \otimes \ldots \otimes e^{i_p} + \textnormal{permutations}) \\ & = V^k\omega_{i_1 \ldots i_p} e^{i_2} \wedge \ldots \wedge e^{i_p}
    \end{aligned}
    \end{equation*}

\section{Volume}   

    A polyhedron in $\mathcal M$ is defined by $n$ linearly independent vectors and its volume is a number. Therefore it is natural to associate an n-form, given the additional antysymmetric property, i.e. to vanish if two vectors are linearly dependent. In a coordinate basis, the $n$ vectors are
    \begin{equation*}
        \Delta x_k = dx^i_{(k)} \pdv{}{x^i}
    \end{equation*}
    and the n-form is 
    \begin{equation*}
        \omega = f e^1 \wedge \ldots \wedge e^n
    \end{equation*}
    Putting together, the volume of an infinitesimal polyhedron is 
    \begin{equation*}
        \omega (\Delta x_{(1)}, \ldots, \Delta x_{(n)}) = f e^i(\Delta x_{(1)}) \ldots e^n(\Delta x_{(n)}) + \textnormal{permutations}
    \end{equation*}
    Choosing coordinate basis, 
    \begin{equation*}
    \begin{aligned}
        \omega (\Delta x_{(1)}, \ldots, \Delta x_{(n)}) & = f dx^1_{(1)} \ldots dx^n_{(n)} + 0 + \ldots + 0 \\ & = f dx^1_{(1)} \ldots dx^n_{(n)} \\ & = dV
    \end{aligned}
    \end{equation*}

    Introducing a lattice of charts, the volume of a region $U \subseteq \mathcal M$ is the integral 
    \begin{equation*}
        V(U) = \int_{\phi(U)} f dx^1 \ldots dx^n = \int_U \omega 
    \end{equation*}

    It is a scalar, since under a change of coordinates $y^i(x^i)$
    \begin{equation*}
        V = \int_U \omega = \int_{\phi'(U)} f(y) J(y) d^n y
    \end{equation*}
    where $J$ is the determinant of the jacobian, which shows that the volume is coordinate independent. 

\section{Area}

    In a submanifold of dimension $n-1$, the infinitesimal area of an hypersurface uses a (n-1)-form, taken by contracting the n-form of the volume with a vector $v \in T_P^{(\mathcal M)} \notin T_P^{(\mathcal S)}$ which is $A = \omega(v, \ldots)$. 

    The infinitesimal area, choosing coordinate basis, is
    \begin{equation*}
    \begin{aligned}
        \omega (v, \omega_{(1)}, \ldots, \omega_{(n-1)}) & = A(\omega_{(1)}, \ldots, \omega_{(n-1)}) \\ & = v f e^1(\omega_{(1)}) \wedge \ldots \wedge e^{n-1} (\omega_{(n-1)}) \\ & = v f dx^1 \ldots dx^{n-1} \\ & = dA
    \end{aligned}
    \end{equation*}
    and the area of a portion $\Sigma \subseteq \mathcal S$ is 
    \begin{equation*}
        A(\Sigma) = \int_{\phi(\Sigma)} f v dx^1 \ldots dx^{n-1} = \int_{\Sigma} A
    \end{equation*}

    
    It is a scalar, since under a change of coordinates $y^i(x^i)$
    \begin{equation*}
        A' = J^{(n-1)}A
    \end{equation*}
    where $J^{(n-1)}$ is the determinant of the jacobian restricted to the image of $\Sigma$. 

\section{Integrating with the metric}

    In a point $P$, the metric can be put in canonical form 
    \begin{equation*}
        g_{ij} (P) = \pm \delta_{ij}
    \end{equation*}

    The natural volume n-form is 
    \begin{equation*}
        \omega_g = e^1 \wedge \ldots \wedge e^n
    \end{equation*}
    
    Under a local change of coordinates $y^i(x^i)$ 
    \begin{equation*}
        \omega_g = J {\omega'}_g = J dy^1 \wedge \ldots \wedge dy^n
    \end{equation*}

    Using $g' = \Lambda^T g \Lambda$
    \begin{equation*}
        \det g' = \det (\Lambda^T g \Lambda) = \det g \det^2 \Lambda = \det g J^2 = \pm J^2
    \end{equation*}
    where $J = \sqrt{|\det g'|}$.

    Hence, the volume of $U$ becomes 
    \begin{equation*}
        V(U) = \int_U \omega_g = \int_{\phi(U)} \sqrt{|\det g'|} dy^1 \ldots dy^n
    \end{equation*}

    Similarly, the natural area (n-1)-form is 
    \begin{equation*}
        A_g = \omega_g (\ldots, e_n) = e^1 \wedge \ldots \wedge e^{n-1}
    \end{equation*}
    and the area of a portion $\Sigma$ is 
    \begin{equation*}
        A = \int_{\Sigma} A_g = \int_{\phi(\Sigma)} \sqrt{|\det g^{(n-1)}|} dx^1 \ldots dx^{n-1}
    \end{equation*}
    where the metric locally is 
    \begin{equation*}
        g_{ij} = \begin{bmatrix}
            g_{ij}^{(n-1)} & 0 \\
            0 & \pm 1
        \end{bmatrix}
    \end{equation*}

\section{Differential forms}

    The exterior derivative of a p-form 
    \begin{equation*}
        \omega = \omega_{i_1 \ldots i_p} dx^{i_1} \wedge \ldots \wedge dx^{i_p}
    \end{equation*}
    is 
    \begin{equation*}
        d \omega = (\partial_k \omega_{i_1 \ldots i_p}) dx^k \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_p}
    \end{equation*}

    It satisfies the following properties 
    \begin{enumerate}
        \item addition, i.e.
            \begin{equation*}
                d(\omega + \sigma) = d\omega + d\sigma
            \end{equation*}
        \item wedge product, i.e.
            \begin{equation*}
                d(\omega \wedge \sigma) = d\omega \wedge \sigma + (-1)^p \omega \wedge d\sigma
            \end{equation*}
        \item vanishing boundary, i.e.
            \begin{equation*}
                d (d\omega) = 0
            \end{equation*}
    \end{enumerate}

    For a function, this is the differential 
    \begin{equation*}
        df = \partial_i f dx^i = df
    \end{equation*}
    such that 
    \begin{equation*}
        d(df) = \partial_i \partial_j f dx^i \wedge dy^j = 0
    \end{equation*}


