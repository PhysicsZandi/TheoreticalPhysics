\part{Manifolds}

\chapter{Manifolds and tensors}

\section{Differentiable Manifolds}

    A differential manifold $\mathcal M$ is a topological space which looks locally like $\mathbb R^N$.

    In a topological space, the notions of contiguity and continuity are well defined. A topological space $(\mathcal M, ~\{A_i\})$ is a set of points $\mathcal M$ in which is defined a family of open sets $\{A_i\}$ such that $\{ \emptyset, ~\mathcal M, ~ \cup_i A_i, ~ \cap_{i < \infty} A_i\} \in \{A_i\}$. In particular, an Haussdorf space has the property that $\forall P, ~Q \in \mathcal M \quad \exists U \in P, ~V \in Q \quad \colon \quad U \cap V = \emptyset$. Two points are contiguous if they belong to the same open subset, called neighbourhood. A map is an application $\phi \colon D \subset \mathcal M \rightarrow \mathbb R^n$. In a topological space, a map is continuous if maps open sets into open sets. 

    A chart is a pair $A, ~\phi$, where $A \subset \mathcal M$ and $\phi \colon A \rightarrow \mathbb R^n$ invertible continuous, which associates a set of $n$ real coordinates $x^i = \phi$ for the open set $A$. An atlas is a colection of charts that covers entirely the manifold $\mathcal A = \{\{(A_i ~\phi_i)\} \colon \cup_i A_i \supseteq \mathcal M \} $. A consistency map between two charts $\phi_1$ and $\phi_2$, over a point $P \in A_1 \cap A_2$, is $\phi \colon \phi(A_2) \subseteq \mathbb R^n \rightarrow \psi(A_2) \subseteq \mathbb R^n $ invertible such that $\psi(\phi_1(P)) = \phi_2(P)$ or $(\phi_2^{-1} \circ \psi \circ \phi_1) = \mathbb I$ or, equivalently, $\psi^{-1} (\phi_2(P)) = \phi_1(P)$ or $(\phi_1^{-1} \circ \psi \circ \phi_2) = \mathbb I$. $\psi$ is a change of coordinates in $\mathbb R^n$. It follows that the dimension $n$ must be the same for all charts, hence it is the dimension of the manifold. If $\psi \in C^p(\mathbb R^n)$, the manifold is a p-differentiable manifold.

    A manifold is an equivalence class of atlases, where two atlases are equivalent if there exists a bijective correspondence between them.

\section{Curves}

    A curve is a continuous map $\gamma \colon I \subseteq \mathbb R \rightarrow \mathcal M$. Introducing a chart $\phi \circ \gamma \colon I \subseteq \mathbb R \rightarrow \mathbb R^n$, or $x^i = x^i(\lambda)$, where $\lambda$ is a real parameter. If $x^i(\lambda) \in C^p(\mathbb R)$, then $gamma$ is p-differentiable. A reparameterization $\gamma' = \gamma'(\gamma)$ defines a different curve, although the images of the curves coincide.

\section{Scalars}

    A function is a map $f \colon \mathcal M \rightarrow \mathbb R$. Introducting a chart $f \circ \phi^{-1} \colon \mathbb R^n \rightarrow \mathbb R$, or $f = f(x^i)$. If $\phi'$ is another chart, then $f'(x'(P)) = f(x(P))$, showing that it is indeed a scalar. 

\section{Vectors}

    A vector at a point $P \in \mathcal M$ is a map that associates to the derivative to a function defined in a neighbourhood of $P$ $v_{\gamma} \colon f \rightarrow v_\gamma(f) = \dvin{f}{\lambda}{\lambda_P} \in \mathbb R$, where $\gamma(\lambda_P) = P$. Introducing a chart 
    \begin{equation*}
    \begin{aligned}
        v_{\gamma, ~P} (f) & = \dvin{(f \circ \gamma)}{\lambda}{\lambda_P} \\ & = \dv{}{\lambda} (f \circ \mathbb I \circ \gamma) \Big\vert_{lambda_P} \\ & = \dv{}{\lambda} (f \circ \phi^{-1} \circ \phi \circ \gamma) \Big\vert_{lambda_P} \\ & = \dv{}{\lambda} (f(x^i) \circ x^i(\lambda)) \Big\vert_{\lambda_P} \\ & = \dv{}{\lambda} f(x^i(\lambda)) \Big\vert_{\lambda_P} = \pdv{f}{x^i} \dv{x^i}{\lambda}
    \end{aligned}
    \end{equation*}
    and since it is true $\forall f$
    \begin{equation}\label{vector}
        v_\gamma = dv{}{\lambda} = \dv{x^i}{\lambda} \pdv{}{x^i}
    \end{equation}
    which means that a vector is the tangent to a curve $\gamma$ at a point $P$.

    By definition a vector is a linear functional
    \begin{equation*}
        v_\gamma (af + bg) = \dv{}{\lambda} (af+bg) = a \dv{f}{\lambda} + b \dv{g}{\lambda}
    \end{equation*}

    From~\eqref{vector}
    \begin{equation*}
        v = \underbrace{\dv{x^i}{\lambda}}_{v_i} \underbrace{\pdv{}{x^i}}_{e_i} = v^i e_i
    \end{equation*}
    where $v^i$ are the components and $e_i$ are the coordinate basis vectors, whose vectors tangent to the coordinate line defined by constant $x^j$ for $i \neq j$.

    Under a change of coordinates
    \begin{equation*}
        y^i = y^i(x^j)
    \end{equation*}
    components transform by 
    \begin{equation*}
        v^i = \dv{x^i}{\lambda} = \pdv{x^i}{y^{j'}} \dv{y^{j'}}{\lambda} = \pdv{x^i}{y^{j'}} v^{j'}
    \end{equation*}
    and basis vectors transform by 
    \begin{equation*}
        e_i = \pdv{}{x^i} = \pdv{y^{j'}}{x^i} \pdv{}{y^{j'}} = \pdv{y^{j'}}{x^i} e_{j'}
    \end{equation*}

    Remember that components transform but basis change too, inversely, then the vector remains the same.

    A vector field in an open set $U \subseteq \mathcal M$ is map from each point $P \in U$ into a vector $v(P)$. Introducing a chart, $v (x^i) = v \circ \phi^{-1}$.

    The coordinate vectors $e_i = \pdv{}{x^i}$ form a basis of a linear space composed by all the vectors tangent to a point $P$, called the tangent space $T_P$.

    \begin{proof}
    First, every tangent vector can be expressed as linear combination.
    
    Consider two curves, parametrized by $\lambda$ and $\sigma$, across a point $P$ which generate two vectors $v = \pdv{}{\lambda}$ and $w = \dv{}{\sigma}$. Hence, a generic linear combination of them
    \begin{equation*}
        a v + b w = a \dv{}{\lambda} + b \dv{}{\sigma} = a \pdv{x^i}{\lambda} \pdv{}{x^i} + b \dv{x^i}{\sigma} \pdv{}{x^i} = \Big ( a \pdv{x^i}{\lambda} + b \dv{x^i}{\sigma} \Big ) \pdv{}{x^i} = \Big ( a \pdv{x^i}{\lambda} + b \dv{x^i}{\sigma} \Big ) e_i
    \end{equation*}
    Since there are $n$ coordinates $x^i$, we have $n$ indipendent curves.

    Second, the coordinate basis vectors are linearly independent. 
    
    The determinant of the jacobian matrix of $y^i = y^i(x^j)$ must not vanish
    \begin{equation*}
        \det J = \det 
        \begin{pmatrix}
            \pdv{y^1}{x^1} & \cdots & \pdv{y^1}{x^n} \\
            \cdots & \cdots & \cdots \\
            \pdv{y^n}{x^1} & \cdots & \pdv{y^n}{x^n} \\
        \end{pmatrix}
    \end{equation*}
    hence, there are $n$ columns (or rows) which are linearly independent and also $n$ basis vector
    \begin{equation*}
        e_i = \pdv{}{x^i} = \pdv{y^j}{x^i} \pdv{}{y^j}
    \end{equation*}

    \end{proof}

    It is important to remark that coordinate basis vectors at different points belong to different tangent space and cannot be linearly combined.

\section{Fiber bundles}

    A tangent bundle is the set of all tangent space at each point together with the manifold itself $\mathcal T \mathcal M = \{\mathcal M, ~\{T_P \colon \forall P \in \mathcal M \}\}$. It can be shown that $\mathcal T \mathcal M$ is a manifold too.

\section{Exponential map}

    An integral curve $\gamma = \gamma(\lambda)$ of a vector field $V$ is the curve which as tangent vector $\dv{}{\lambda}$ has the element of $V$ in $P \in \gamma$, i.e. 
    \begin{equation*}
        V = \dv{}{\lambda}
    \end{equation*}

    Introducing a point $P_0$ and a chart $x^i$
    \begin{equation} \label{cauchy}
        \begin{split}
            V^i(\lambda) = \dv{x^i(\lambda)}{\lambda} \\
            x^i(P_0) = x^i(\lambda_0)
        \end{split}
    \end{equation}
    which are a system of $n$ Cauchy problems and the components of $V$ at an arbitrary point $ P = \phi^{-1}(x^i(\lambda))$ are $V(P) = V^i(x^j(\lambda)) \pdv{}{x^i} = V^i(\lambda) \pdv{}{x^i}$. 
    
    Theorems of calculus in $\mathbb R^n$ ensure that locally tha solution of~\eqref{cauchy} always exists, which is indeed the integral curve $\gamma(\lambda)$.

    Formally, the solution of~\eqref{cauchy} is the exponential map
    \begin{equation*}
        x^i(\lambda) = \exp((\lambda - \lambda_0)V) x^i \Big\vert_{\lambda_0}
    \end{equation*}
    which describes the flow of $V$ in a neighbourhood of $P$.

    \begin{proof}
        Let $V = \dv{}{\lambda}$ be a vector fields with integral curve $\gamma = \gamma (\lambda)$. Introducing a chart $x^i$ and Taylor expanding around $P_0$ along $\gamma$
        \begin{equation*}
        \begin{aligned}
            x^i(\lambda_0 + \epsilon) & = x^i(\lambda_0) + \epsilon \dv{x^i}{\lambda} \Big\vert_{\lambda_0} + \frac{\epsilon^2}{2} \dvd{x^i}{\lambda} \Big\vert_{\lambda_0} + \ldots \\ & = \Big (1 + \epsilon \dv{}{\lambda} \Big\vert_{\lambda_0} + \frac{\epsilon^2}{2} \dvd{}{\lambda} \Big\vert_{\lambda_0} + \ldots \Big) ~ x^i(\lambda_0) \\ & = \exp(\epsilon \dv{}{\lambda}) ~ x^i \Big\vert_{\lambda_0} \\ & = \exp(\epsilon V) ~ x^i \Big\vert_{\lambda_0} 
        \end{aligned}
        \end{equation*}
    \end{proof}
    
    For an arbitrary function $f$ in a neighbourhood of $P$
    \begin{equation*}
        f(\lambda_0 + \epsilon) = \exp \Big(\epsilon \dv{}{\lambda} \Big) ~ f \Big\vert_{\lambda_0} = \exp (\epsilon V ) ~ f \Big\vert_{\lambda_0}
    \end{equation*}

\section{Lie brackets}

    Introducing a chart $x^i$, the Lie brackets of two vector fields $V = \dv{}{\lambda} = v^i \pdv{}{x^i}$ and $W = \dv{}{\mu} = w^i \pdv{}{x^i}$ are 
    \begin{equation*}
    \begin{aligned}
        [V, ~W] & = \dv{}{\lambda} \dv{}{\mu} - \dv{}{\mu} \dv{}{\lambda} \\ & = v^i \pdv{}{x^i} \Big( w^j \pdv{}{x^j} \Big) - w^i \pdv{}{x^i} \Big( v^j \pdv{}{x^j} \Big) \\ & = \cancel{v^i w^j \pdv{}{x^i} \pdv{}{x^j}} + v^i \pdv{w^j}{x^i} \pdv{}{x^j} - \cancel{v^j w^i \pdv{}{x^i} \pdv{}{x^j}} - w^i \pdv{v^j}{x^i} \pdv{}{x^j} \\ & = \Big ( v^i \pdv{w^j}{x^i} - w^i \pdv{v^j}{x^i} \Big ) \pdv{}{x^j} 
    \end{aligned}
    \end{equation*}
    where it is used the facf that the partial derivatives commute. Hence the commutator of two vectors is still a vector. 

    The geometrical meaning of the commutator is the following: consider two vector fields $V = \dv{}{\lambda}$ and $W = \dv{}{\mu}$. Using the exponential map, the coordinates of A, moving before along $V$ and then along $W$, are 
    \begin{equation*}
        x^i(A) = \exp \Big ( \epsilon_2 \dv{}{\mu} \Big) \exp \Big ( \epsilon_1 \dv{}{\lambda} \Big) ~ x^i \Big\vert_P
    \end{equation*}
    whereas the coordinates of B, moving before along $W$ and then along $Y$, are 
    \begin{equation*}
        x^i(B) = \exp \Big ( \epsilon_1 \dv{}{\lambda} \Big) \exp \Big ( \epsilon_2 \dv{}{\mu} \Big) ~ x^i \Big\vert_P
    \end{equation*}
    Computing the difference
    \begin{equation*}
        x^i(B) - x^i(A) = \epsilon_1 \epsilon_2 \Big [\dv{}{\lambda}, ~\dv{}{\mu} \Big ] ~ x^i \Big\vert_P + O(\epsilon^3)
    \end{equation*}

    Hence, if the commutator does not vanish, the final points are different $A \neq B$ and the path $PA \cup PB$ does not close. 

    A sufficient and necessary condition for a set of fields to be coordinate vectors is that their commutator vanishes. 
    \begin{proof}
    First, the sufficient condition. Consider two coordinate vector fields $V = \pdv{}{x^1}$ and $W = \pdv{}{x^2}$. Then $v^i = \delta^i_{\phantom i 1}$, $w^j = \delta^j_{\phantom j 2}$ and the commutator vanishes, since the derivative of a constant is so. 

    Second, the necessary condition. Consider two commuting vector fields $V = \pdv{}{x^1}$ and $W = \pdv{}{x^2}$. Suppose also that they are linearly independent
    \begin{equation} \label{proof2}
        a V(P) + b W(P) = 0 \quad \iff \quad a = b = 0.
    \end{equation}
    Introducing a chart $x^i$, moving from $P$ along $V$ by $\Delta \lambda = \alpha$ to a point $R$
    \begin{equation*}
        x^(R) = \exp \Big (\alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P
    \end{equation*}
    and then along $W$ by $\Delta \mu = \beta$ to a point $Q$
    \begin{equation}\label{proof1}
        x^(Q) = \exp \Big (\beta \dv{}{\mu} \Big) \exp \Big (\alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P
    \end{equation}
    If $\alpha$ and $\beta$ are coordinates, the corresponding basis vectors are $\pdv{}{\alpha} = \pdv{x^i}{\alpha}$ and $\pdv{}{\beta} = \pdv{x^i}{\beta}$.
    Hence, using~\eqref{proof1}
    \begin{equation*}
    \begin{aligned}
        \pdv{x^i}{\alpha} & = \pdv{}{\alpha} \Big ( \exp \Big( \beta \dv{}{\mu} \Big) \exp \Big( \alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P \Big) \\ & = \exp \Big( \beta \dv{}{\mu} \Big) \pdv{}{\alpha} \Big ( \exp \Big( \alpha \dv{}{\lambda} \Big) ~ x^i \Big\vert_P \Big) \\ & = \exp \Big( \beta \dv{}{\mu} \Big) \exp \Big( \alpha \dv{}{\lambda} \Big) ~ \dv{x^i}{\lambda} \Big\vert_P
    \end{aligned}
    \end{equation*}
    and, similarly, 
    \begin{equation*}
        \pdv{x^i}{\beta} = \exp \Big( \beta \dv{}{\mu} \Big) \exp \Big( \alpha \dv{}{\lambda} \Big) ~ \dv{x^i}{\mu} \Big\vert_P
    \end{equation*}
    This shows that $\pdv{}{\alpha}$ and $\pdv{}{\beta}$ are respectively the vector fields $\dv{}{\lambda}$ and $\pdv{}{\mu}$ evaluated in $Q$, using the fact that the commutator is vanishing. Then the determinant of the Jacobians is not zero
    \begin{equation*}
        \det J = \det \begin{bmatrix}
            \pdv{x^1}{\alpha} & \pdv{x^2}{\alpha} \\
            \pdv{x^1}{\beta} & \pdv{x^2}{\beta} \\
        \end{bmatrix} \neq 0
    \end{equation*}
    since we assumed the linearly independence of $\dv{}{\lambda}$ and $\dv{}{\mu}$.
    \end{proof} 

\section{1-forms}

    A 1-form is a linear functional $w$ acting on a vector $w \colon T_P \rightarrow \mathbb R$ such that $w(\alpha v + \beta u) = \alpha w(v) + \beta w(u)$ and $(\alpha w + \alpha z)(v) = \alpha w(v) + \beta z(v)$. Linearity implies that the action of a 1-form is completely determined by the action on a basis of $T_P$. 1-forms acting on the same $T_P$ form a linear space $T^*_P$, called the cotangent space. A cotangent bundle is the set of all cotangent space at each point together with the manifold itself $\mathcal T^* \mathcal M = \{\mathcal M, ~\{T^*_P \colon \forall P \in \mathcal M \}\}$. A 1-form field is a map associates a 1-form of $T^*P$ to each point $P \in \mathcal M$.

    One way of thinking 1-forms is in the following way: given an arbitrary function, a vector field is defined by $V(f) = \dv{f}{\lambda}$ whereas given an arbitrary vector field, a 1-form is defined by $V(f) = \dv{f}{\lambda} = df (\dv{}{\lambda})$. The difference is the in the former $V$ is fixed and $f$ is arbitrary, whereas in the latter $f$ is fixed and $V$ is arbitrary. Introducing a chart $x^i$
    \begin{equation*}
        \dv{f}{\lambda} = \pdv{f}{x^i} \dv{x^i}{\lambda} = \nabla_i f \dv{x^i}{\lambda} = df_i \dv{x^i}{\lambda}
    \end{equation*}
    where $df_i$ are the components of the 1-form $df$, called the gradient of $f$.
    
    Geometrically, the gradient of a function can be seen as the number of contour lines that a vector $V$ crosses in a neighbourhood of $P$. Generalizing for 1-forms, they can be seen as a set of level surfaces whose action on a vector is the number of surface the vector crosses. 

    Let $\{e_i\}$ be a basis of $T_P$. A basis of $T^*_P$ is not related to it, however it is convenient to choose the dual basis, which completely defined a basis of $T^*P$ by a basis in $T_P$ in the following way
    \begin{equation}\label{dual}
        e^i(e_j) = \delta^i_{\phantom i j}
    \end{equation}
    or, equivalently, applying it to a vector $v$
    \begin{equation*}
        e^i(v) = e^i(v^j e_j) = v^j e^i(e_j) = v^j \delta^i_{\phantom i j} = v^i
    \end{equation*}

    Consequently, $\mathcal M$, $T_P$ and $T^*_P$ have the same dimension $n$. $\{e^i\}$ are actually a basis of $T^*_P$, since given an arbitrary 1-form $q$
    \begin{equation*}
        q(v) = q(v^i e_i) = v^i q(e_i) = v^i q_i
    \end{equation*}
    and
    \begin{equation*}
        v^i q_i = q_i e^i(v)
    \end{equation*}

    Remember that under a change of coordinates, vectors or 1-forms do not change, only their components and basis change, the latter inversely.

    Differentials are the basis 1-form dual to the coordinate basis vectors 
    \begin{equation*}
        \dv{f}{\lambda} = \pdv{f}{x^i} \dv{x^i}{\lambda} = \pdv{f}{x^i} dx^i \Big(\dv{x^j}{\lambda} \pdv{}{x^j} \Big) = \pdv{f}{x^i} \dv{x^j}{\lambda} dx^i \Big ( \pdv{}{x^j} \Big) = \pdv{f}{x^i} \pdv{x^j}{\lambda} \delta^i_{\phantom i j} = \pdv{f}{x^i} \pdv{x^i}{\lambda}
    \end{equation*}
    where it has been used the dual basis.

\section{Tensors}  

    A tensor (n, m) at $P$ is a linear functional that maps $n$ 1-forms and $m$ vectors into a real number 
    \begin{equation*}
        T \colon \underbrace{T^*_P \otimes \cdots \otimes T^*_P}_{n~times} \otimes \underbrace{T_P \otimes \cdots \otimes T_P}_{m~times} \rightarrow \mathbb R
    \end{equation*}

    A tensor can be also seen as the outer product of 1-forms and vectors. A tensor (1, 0) is a vector and a tensor (0, 1) is a 1-form. A tensor (n, m) can be written in terms of the dual basis
    \begin{equation*}
        T = T^{i_1 \cdots i_n}_{j_1 \cdots j_m} e_{i_1} \otimes \cdots \otimes e_{i_n} \otimes e^{j_1} \otimes \cdots \otimes e^{j_m}
    \end{equation*}
    where the components are
    \begin{equation*}
        T^{i_1 \cdots i_n}_{j_1 \cdots j_m} = T(e^{i_1}, ~\cdots, ~e^{i_n}, ~e_{j_1}, ~\cdots, ~e^{j_m})
    \end{equation*}

    A change of basis is determined by a $4 \times 4$ non-degenerate matrix $\Lambda \in GL(n)$. On a vector basis, it acts as 
    \begin{equation}\label{proof4}
        {e'}_j = \Lambda^i_{\phantom i j} e_i
    \end{equation}
    This transformation has no effects on the dual space, however, in order to keep the duality of the basis, it must induce a transformation with the inverse matrix 
    \begin{equation*}
        {e'}^j = \Lambda^j_{\phantom j i} e^i
    \end{equation*}

    \begin{proof}
        Recalling~\eqref{dual}, to preserve the duality, also the transformed dual basis must obey 
        \begin{equation}\label{proof3}
            {e'}^i ({e'}_j) = \delta^i_{\phantom i j}
        \end{equation}
        Hence, given an arbitrary transformation matrix, 
        \begin{equation*}
            {e'}^i = M^i_{\phantom i k} e^k
        \end{equation*}
        and putting into~\eqref{proof3}, using\eqref{proof4}
        \begin{equation*}
            \delta^i_{\phantom i j} = {e'}^i ({e'}_j) = M^i_{\phantom i k} e^k (\Lambda^l_{\phantom l j} e_l) = M^i_{\phantom i k} \Lambda^l_{\phantom l j} e^k(e_l) = M^i_{\phantom i k} \Lambda^l_{\phantom l j} \delta^k_{\phantom k l} = M^i_{\phantom i k} \Lambda^k_{\phantom k j}
        \end{equation*}
        then, $M$ must satisfy
        \begin{equation*}
            M^i_{\phantom i k} \Lambda^k_{\phantom k j} = \delta^i_{\phantom j} 
        \end{equation*}
        and it is indeed the inverse matrix.
    \end{proof}

    It is possible to perform several operations on tensors at $P$:
    \begin{enumerate}
        \item scalar multiplication, i.e. 
            \begin{equation*}
                S^{(n,m)} = a T^{(n,m)} \quad \forall a \in \mathbb R
            \end{equation*}
        \item addition, i.e. 
            \begin{equation*}
                S^{(n,m)} = T^{(n,m)} + Q^{(n,m)}
            \end{equation*}
        \item outer product, i.e. 
            \begin{equation*}
                S^{(n+p,m+q)} = T^{(n,m)} \otimes Q^{(p,q)}
            \end{equation*}
        \item saturation with 1-forms, i.e. 
            \begin{equation*}
                T^{(n-1,m)} = T^{(n,m)} (\ldots, ~w, ~\ldots)
            \end{equation*}
        \item saturation with vector, i.e. 
            \begin{equation*}
                T^{(n,m-1)} = T^{(n,m)} (\ldots, ~v, ~\ldots)
            \end{equation*}
    \end{enumerate}
    The last two can be generalised to an arbitrary saturation of a $(n,m)$ tensor with a $(p<n, q<m)$ tensor. 

    For a change of basis in the tangent space to correspond a change of coordinates on the manifold, the transformation matrix must obey the condition
    \begin{equation}\label{proof8}
        \pdv{\Lambda^j_{\phantom j i}}{x^k} = \pdv{\Lambda^j_{\phantom j k}}{x^i}
    \end{equation}

    \begin{proof}
        Consider two charts $x^i$ and $y^i$ that overlap at $P$. The transformation matrix between basis is 
        \begin{equation*}
            \Lambda^i_{\phantom i j} = \pdv{x^i}{y^j}
        \end{equation*}
        and the inverse is 
        \begin{equation*}
            \Lambda^j_{\phantom j i} = \pdv{y^j}{x^i}
        \end{equation*}

        If we move continuously to another point $Q$ insider the charts, the matrix transformation will become a field $\Lambda(Q) = \Lambda(x^i(Q)) = \Lambda(y^i(Q))$ and, since the partial derivatives commute
        \begin{equation*}
            \pdv{\Lambda^j_{\phantom j i}}{x^k} = \pdv{}{x^k} pdv{y^j}{x^i} = \pdv{}{x^i} pdv{y^j}{x^k} = \pdv{\Lambda^j_{\phantom j k}}{x^i}
        \end{equation*}
    \end{proof}

\section{Metric tensor}

    The notions of lenght and angles on a manifold can be introduced with the metric tensor.

    A metric tensor $g$ is a $(2,0)$ tensor which maps two vectors into a real number, satisfying the following properties
    \begin{enumerate}
        \item symmetry, i.e.
            \begin{equation*}
                g(v,w) = g(w, v) = g(v^i e_i, w^j e_j) = g(e_i, e_j) v^i w^j = g_{ij} v^i v^j \quad \forall v, ~w \in T_P
            \end{equation*}
        \item non-degeneracy, i.i 
            \begin{equation*}
                g(v, w) = 0 \quad \forall w \in T_P \quad \iff \quad v=0 
            \end{equation*}
            or, equivalenlty, if $\det g_{ij} \neq 0$
    \end{enumerate}

    A metric tensor defines a scalar product 
    \begin{equation*}
        g(v, w) = v \cdot w
    \end{equation*}
    and introduces the notions of norm of a vector 
    \begin{equation*}
        v^2 = g(v,v) = v \cdot v = g_{ij} v^i v^j
    \end{equation*}
    and angle between two vectors 
    \begin{equation*}
        g(v, w) = v w \cos \theta
    \end{equation*}
    Although, the latter only with Riemannian metrics.

    The metric tensor, under a change of basis $\Lambda$, change 
    \begin{equation*}
        g' = \Lambda^T g \Lambda
    \end{equation*}
    where ${g'}_{ij} = g({e'}_i, {e'}_j)$. Since it is symmetric, it can be always possible to find two matrices $O^{-1} = O^{T}$ and $D = D^T = diag(\frac{1}{\sqrt{|g_{ii}^{(diag)}|}})$ such that 
    \begin{equation*}
        g' = D^T O^T g O D = D g^{(diag)} D 
    \end{equation*} 
    and put in canonical form
    \begin{equation*}
        {g'}_{ij} = \pm \delta_{ij}
    \end{equation*}
    which defines an orthonormal basis at $P$, i.e. $g(e_i, e_j) = \pm \delta_{ij}$. 

    The $\pm$ cannot be eliminated and the sum of the diagonal element is called the signature. A sign inversion does not affect the signature. The diagonal elements can classify the metric in the following way:
    \begin{enumerate}
        \item Riemannian metric, i.e. all of the same sign
        \item pseduo-Riemannian metric, i.e. both signs appear (Lorentzian metric if one is of one kind and all the others of the other kind)
    \end{enumerate}

    Metric tensors define a map between $T_P$ and $T^*_P$, to lower indices and the inverse to raise them. Infact, a vector $v \in T_P$ can be mapped into a 1-form
    \begin{equation*}
        v_i = v(e_i) = g(v^j e_j, e_i) = v^i g(e_j, e_i) = v^i g_{ij}
    \end{equation*}
    and a 1-form $w \in T^*_P$ can be mapped into a vector 
    \begin{equation*}
        w^i = e^i(w) = g(e^i, w_j e^j) = w_j g(e^i, e^j) = w_j g^{ij}
    \end{equation*}
    Consequently, at $P$ a vector and a 1-form are equivalent.

    
    The inverse metric tensor in defined by 
    \begin{equation*}
        g^{-1}_{ij} = g^{ij} \quad g_{ij} g^{jk} = \delta^k_{\phantom k i}
    \end{equation*}

    If the metric is in canonical form, the dual basis will be orthonormal. 

    A metric tensor field is a map that associates each point of $\mathcal M$ into a metric tensor. The manifold becomes a metric manifold $(\mathcal M, ~g)$. The metric tensor field in terms of coordinate vectors and dual basis is 
    \begin{equation*}
        g(x) = g_{ij}(x) dx^i \otimes dx^j
    \end{equation*}
    which is written as line element
    \begin{equation*}
        ds^2 = g_{ij}(x) dx^i dx^j
    \end{equation*}

    Consider the integral curve $\gamma$ of a vector field $v = \dv{}{\lambda}$. The scalar infinitesimal displacement along $v$ is 
    \begin{equation*}
        ds^2 = dx \cdot dx = g(dx, dx) = g(v d\lambda, v d\lambda) = g(v,v) d\lambda^2
    \end{equation*}

    Integrating along $\gamma$, the length of the path between $\lambda_1$ and $\lambda_2$ is 
    \begin{equation*}
        s(\lambda_1, \lambda_2) = \integ{\lambda_1}{\lambda_2}{\lambda} \sqrt{g(v,v)} = \integ{\lambda_1}{\lambda_2}{\lambda} \sqrt{g_{ij}(\lambda) v^i(\lambda) v^j(\lambda)}
    \end{equation*}

    Introducing a chart $x^i$, 
    \begin{equation*}
        s(\lambda_1, \lambda_2) = \integ{\lambda_1}{\lambda_2}{\lambda} \sqrt{g_{ij}(\lambda) \dv{x^i}{\lambda} \dv{x^j}{\lambda}}
    \end{equation*}


    It is always possible to find a change of coordinate that put the metric tensor field in the locally canonical form
    \begin{equation*}
        g_{ij} (x) = \pm \delta_{ij} + \frac{1}{2} \pdvd{g_{ij}}{x^k}{x^l} \Big\vert_{x_P} \delta x^k \delta x^l
    \end{equation*}
    which means to find a locally orthogonal coordinates $x^i$ such that $g(\pdv{}{x^i}, \pdv{}{x^j}) = \pm \delta_{ij}$. However, this holds only locally, not on the entire manifold.

    \begin{proof}
        Around $P$, the metric tensor field $g_{ij}$ can be Taylor expanded in $x = x_P + \delta x$
        \begin{equation} \label{proof5}
            g_{ij} = g_{ij} (x_P) + \pdv{g_{ij}}{x^k} \Big\vert_{x_P} \delta x^k + \frac{1}{2} \pdvd{g_{ij}}{x^k}{x^l} \Big\vert_{x_P} \delta x^k \delta x^l + \ldots
        \end{equation}
        as well as the transformation matrix 
        \begin{equation}\label{proof6}
            \pdv{x^i}{y^j} (x) = \pdv{x^i}{y^j} (x_P) + \pdv{}{x^k} \pdv{x^i}{y^j} \Big\vert_{x_P} \delta x^k + \frac{1}{2} \pdvd{}{x^k}{x^l} \pdv{x^i}{y^j} \Big\vert_{x_P} \delta x^k \delta x^l + \ldots
        \end{equation}
        and the metric in the new coordinates
        \begin{equation} \label{proof7}
            {g'}_{ij} = {g'}_{ij} (y_P) + \pdv{{g'}_{ij}}{y^k} \Big\vert_{y_P} \delta y^k + \frac{1}{2} \pdvd{{g'}_{ij}}{y^k}{y^l} \Big\vert_{y_P} \delta y^k \delta y^l + \ldots
        \end{equation}

        Using  
        \begin{equation*}
            {g'}_{ij} = \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl}
        \end{equation*}
        then the left-handed side is
        \begin{equation*}
        \begin{aligned}
            \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} & = 
            \Big ( \pdv{x^k}{y^i} + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a + \frac{1}{2} \pdvd{}{x^a}{x^e} \pdv{x^k}{y^i} \delta x^a \delta x^e + \ldots \Big ) \\ & \quad
            \Big ( \pdv{x^l}{y^j} + \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b + \frac{1}{2} \pdvd{}{x^b}{x^f} \pdv{x^l}{y^j} \delta x^b \delta x^f  + \ldots \Big ) \\ & \quad
            \Big ( g_{kl} + \pdv{g_{kl}}{x^c}  \delta x^c + \frac{1}{2} \pdvd{g_{kl}}{x^c}{x^d} \delta x^c \delta x^d + \ldots \Big ) 
            \\ & = 
            \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b g_{kl} 
            + \pdv{x^k}{y^i} \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b \pdv{g_{kl}}{x^c} \delta x^c 
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^c}  \delta x^c \\ & \quad 
            + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a  \pdv{x^l}{y^j} g_{kl} 
            + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a \pdv{}{x^b} \pdv{x^l}{y^j} \delta x^b g_{kl}
            + \pdv{}{x^a} \pdv{x^k}{y^i} \delta x^a  \pdv{x^l}{y^j} \pdv{g_{kl}}{x^c} \delta x^c \\ & \quad 
            + \frac{1}{2} \pdvd{}{x^a}{x^e} \pdv{x^k}{y^i} \delta x^a \delta x^e \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \frac{1}{2} \pdvd{}{x^b}{x^f} \pdv{x^l}{y^j} \delta x^b \delta x^f g_{kl} \\ & \quad 
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \frac{1}{2} \pdvd{g_{kl}}{x^c}{x^d} \delta x^c \delta x^d + \ldots 
            \\ & = \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} + \delta x^a \Big( \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} g_{kl} + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^a} + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} \Big) 
            \\ & \quad + \delta x^a \delta x^b \Big(
            \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b}
            + \pdv{}{x^a} \pdv{x^k}{y^i}\pdv{}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b}
            \\ & \qquad \qquad \qquad + \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \frac{1}{2} \pdvd{g_{kl}}{x^a}{x^b} \Big)
        \end{aligned}
        \end{equation*}

        Comparing infinitesimal of the same order
        \begin{equation*}
            \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} = {g'}_{ij}
        \end{equation*}
        \begin{equation*}
            \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} g_{kl} + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^a} + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} = \pdv{{g'}_{ij}}{y^k}
        \end{equation*}
        \begin{equation*}
        \begin{aligned}
            & \pdv{x^k}{y^i} \pdv{}{x^a} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b}
            + \pdv{}{x^a} \pdv{x^k}{y^i}\pdv{}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{}{x^a} \pdv{x^k}{y^i} \pdv{x^l}{y^j} \pdv{g_{kl}}{x^b} \\ & \quad + \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^k}{y^i} \pdv{x^l}{y^j} g_{kl} 
            + \pdv{x^k}{y^i} \frac{1}{2} \pdvd{}{x^a}{x^b} \pdv{x^l}{y^j} g_{kl}
            + \pdv{x^k}{y^i} \pdv{x^l}{y^j} \frac{1}{2} \pdvd{g_{kl}}{x^a}{x^b} = \frac{1}{2} \pdvd{{g'}_{ij}}{y^k}{y^l}
        \end{aligned}
        \end{equation*}

        Looking at this system of equations, we find 1 degree of freedom for the first one, $n$ for the second one and $n^2$ for the third one. Hence, since $\Lambda$ has $n^2 - 1$ degrees of freedom with $-1$ coming from~\eqref{proof8}, we only have enough degree of freedom to put 
        \begin{equation*}
            {g'}_{ij} (y_P) = \pm \delta_{ij}
        \end{equation*}
        and 
        \begin{equation*}
            \pdv{g_{ij}}{y^k} \Big\vert_{y_P} = 0
        \end{equation*}
        but not enough to put 
        \begin{equation*}
            \pdvd{g_{ij}}{x^k}{x^l} \Big\vert_{y_P} = 0
        \end{equation*}
    \end{proof}